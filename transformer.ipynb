{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "P-Ol12BRl9CL"
      },
      "outputs": [],
      "source": [
        "# !pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NYmGcmbWl3TC"
      },
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.nn.init import xavier_uniform_\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from torchinfo import summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NrSO0Upgl_Vj"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZYuiPoY_7KX2"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ModelConfig:\n",
        "    d_model: int = 512\n",
        "    n_heads: int = 8\n",
        "    n_layers: int = 8\n",
        "    d_ff: int = 2048\n",
        "    attn_dropout: float = 0.1\n",
        "    mlp_dropout: float = 0.1\n",
        "    qkv_bias: bool = False\n",
        "\n",
        "    max_len: int = 2048 # context_len, seq_len\n",
        "    src_vocab_size: int = 1024\n",
        "    tgt_vocab_size: int = 1024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gn0CNAgEupY"
      },
      "source": [
        "# Transzformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGJB7qruExV_"
      },
      "source": [
        "Az architektúra egy kódoló és dekódoló részből áll. A kódoló az $\\mathbf{x} = ( x_1, x_2, \\ldots, x_n )$ bemeneti szekvenciát a $\\mathbf{z} = (z_1, z_2, \\ldots, z_n)$ folytonos reprezentációvá képezi le, majd a dekódoló a $\\mathbf{z}$ reprezentációból és az $\\mathbf{x}$ szekvenciából az $\\mathbf{y} = (y_1, y_2, \\ldots, y_m)$ kimeneti szekvenciát generálja, ahol $n,m \\in \\mathbb{N}^+$ .\n",
        "\n",
        "Minden egyes (idő)lépésben, a kimenet következő elemének generálásához a korábbi kimeneti elemek is hozzájárulnak, ezért a modell **auto-regresszív** (auto-regressive).\n",
        "\n",
        "A kódoló és dekódoló komponensek egymásra épülő rétegek sorozata, amelyek mindegyike figyelem (attention) mechanizmusból és teljesen összekapcsolt előrecsatolt hálózatból (fully connected feed-forward network) áll.\n",
        "\n",
        "Megjegyzés. A bemeneti szekvencia a tokenizálás után előálló sorozat, ahol a szöveg ún. tokenekre van bontva, ami lehet szó, szórész, karakter (vagy bájt). A tokenek egyedi azonosítók, $\\mathbb{N}$ elemei."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2y-iQUldm9TM"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "\n",
        "        self.src_embedding = Embeddings(config.src_vocab_size, config.d_model)\n",
        "        self.tgt_embedding = Embeddings(config.tgt_vocab_size, config.d_model)\n",
        "\n",
        "        self.pe = PositionalEncoding(config)\n",
        "\n",
        "        self.encoder = Encoder(config)\n",
        "        self.decoder = Decoder(config)\n",
        "\n",
        "        self.head = nn.Linear(config.d_model, config.tgt_vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt) -> torch.Tensor:\n",
        "\n",
        "        tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt)\n",
        "\n",
        "        memory = self.encode(src, src_padding_mask)\n",
        "        logits = self.decode(tgt, memory, tgt_mask, tgt_padding_mask)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def encode(self, src, padding_mask):\n",
        "        src = self.pe(self.src_embedding(src))\n",
        "        memory = self.encoder(src, attn_mask = None, padding_mask = padding_mask)\n",
        "        return memory\n",
        "\n",
        "    def decode(self, tgt, memory, attn_mask, padding_mask):\n",
        "        tgt = self.pe(self.tgt_embedding(tgt))\n",
        "        x = self.decoder(tgt, memory, attn_mask, padding_mask)\n",
        "        logits = self.head(x)\n",
        "        return logits\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                xavier_uniform_(p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POKZ8jT5H8jq"
      },
      "source": [
        "# Kódoló"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-NeFLFkH95x"
      },
      "source": [
        "Az eredeti implementációban a kódoló hat egymásra épülő identikus rétegből áll. Minden réteg két alrétegből áll.\n",
        "\n",
        "Az első a többszörös vagy többfejű figyelem (multihead attention) mechanizmus, a második egy teljesen összekapcsolt előrecsatolt hálózat.\n",
        "\n",
        "A szerzők reziduális kapcsolatokat (residual connection, skip connection) alkalmaztak mindkét alréteg körül. A reziduális kapcsolatok közvetlenül biztosítják a gradiensek áramlását a hálózaton keresztül azáltal, hogy egy vagy több réteget átugranak.\n",
        "\n",
        "Utána, réteg normalizálást (layer normalization) alkalmaztak, hogy stabilizálják a tanulási folyamatot. Azaz minden alréteg kimenete a következőképpen számolható:\n",
        "\n",
        "$$\\text{LayerNorm}(x + \\text{Sublayer}(x)),$$\n",
        "\n",
        "ahol a $\\text{Sublayer}(x)$ az alréteg műveletét jelenti. Minden modellbeli alréteg, beleértve a beágyazás réteget, 512 dimenziós kimenetet állít elő."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "L9ik1EkVJDhc"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layers = nn.Sequential(\n",
        "            *[EncoderBlock(config) for _ in range(config.n_layers)]\n",
        "        )\n",
        "\n",
        "    def forward(self, x, attn_mask = None, padding_mask = None):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, attn_mask, padding_mask)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NKb2vd3sm5Ck"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "\n",
        "        self.multi_head_attention = MultiHeadAttention(\n",
        "            config.d_model,\n",
        "            config.n_heads,\n",
        "            config.attn_dropout\n",
        "        )\n",
        "        self.layer_norm1 = LayerNorm(config.d_model)\n",
        "\n",
        "        self.ffd = PositionwiseFeedForward(config)\n",
        "        self.layer_norm2 = LayerNorm(config.d_model)\n",
        "\n",
        "    def forward(self, x, attn_mask = None, padding_mask = None) -> torch.Tensor:\n",
        "        x = self.layer_norm1(\n",
        "            x + self.multi_head_attention(x, x, x, attn_mask=None, padding_mask=padding_mask)\n",
        "        )\n",
        "        x = self.layer_norm2(x + self.ffd(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "z_lvDyOuM8KI"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, features, eps=1e-6) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.a_2 = nn.Parameter(torch.ones(features))\n",
        "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uy-qnluSIhPO"
      },
      "source": [
        "# Dekódoló"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2l9P7z9zIjEa"
      },
      "source": [
        "A dekóder szintén hat egymásra épülő, azonos felépítésű rétegből áll és minden réteg három alréteget tartalmaz.\n",
        "\n",
        "Az első alréteg egy maszkolt többfejű figyelem (masked multi-head attention) mechanizmus, amely biztosítja, hogy az adott pozícióhoz tartozó kimenet csak a saját és a megelőző pozíciókra támaszkodjon.\n",
        "\n",
        "Továbbá, a kimeneti beágyazások egy pozícióval eltolva kerülnek a hálózatba, ami a maszkolással együtt garantálja, hogy az $i$-dik pozícióhoz tartozó predikció csak az $i$-nél kisebb pozíciók kimeneteitől függjön.\n",
        "\n",
        "A második és harmadik alréteg a kódoló alrétegekkel megegyezik, viszont a második többfejű figyelem a kódoló kimenetét is figyelembe veszi.\n",
        "\n",
        "A kódolóhoz hasonlóan minden alréteg köré reziduális kapcsolatot építettek, majd réteg normalizálást alkalmaztak."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DXdRLTP1m8Ka"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layers = nn.Sequential(*[DecoderBlock(config) for _ in range(config.n_layers)])\n",
        "\n",
        "    def forward(self, x, memory, attn_mask, padding_mask = None):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, memory, attn_mask, padding_mask)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "YBBc2XI2m6eh"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "\n",
        "        self.masked_mha = MultiHeadAttention(\n",
        "            config.d_model,\n",
        "            config.n_heads,\n",
        "            config.attn_dropout,\n",
        "            config.qkv_bias\n",
        "        )\n",
        "        self.layer_norm1 = LayerNorm(config.d_model)\n",
        "\n",
        "        self.mha = MultiHeadAttention(\n",
        "            config.d_model,\n",
        "            config.n_heads,\n",
        "            config.attn_dropout,\n",
        "            config.qkv_bias\n",
        "        )\n",
        "        self.layer_norm2 = LayerNorm(config.d_model)\n",
        "\n",
        "        self.ffd = PositionwiseFeedForward(config)\n",
        "        self.layer_norm3 = LayerNorm(config.d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.mlp_dropout)\n",
        "\n",
        "    def forward(self, x, memory, attn_mask, padding_mask = None):\n",
        "        x = self.layer_norm1(x + self.masked_mha(x, x, x, attn_mask, padding_mask))\n",
        "        x = self.layer_norm2(x + self.mha(x, memory, memory, attn_mask = None, padding_mask = padding_mask))\n",
        "        x = self.layer_norm3(x + self.ffd(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7US8bB-hJWzv"
      },
      "source": [
        "# Beágyazási réteg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDajRVeLJXtv"
      },
      "source": [
        "A beágyazási réteg egy tanítható réteg, amely a diszkrét bemenetet (tokeneket) folytonos $d$ dimneziós vektorokká alakítja. Egy egyszerű keresőtábla, amely rögzített szótárhoz (vocab) tárol beágyazásokat, vagyis vektortérbeli reprezentációkat. A szótár minden egyes eleméhez egyedi vektor tartozik.\n",
        "\n",
        "A beágyazási rétegekben, a súlyok $\\sqrt{d}$-vel vannak skálázva és a dimenziója megegyezik a modell belső rétegeinek dimenziójával, vagyis $d$-vel. Ha a bemeneti szekvencia maximális hossza $n$, akkor $n$ új beágyazásvektort tanulunk meg - minden token pozícióra egyet.\n",
        "\n",
        "Megjegyzés. A szótár angolul könnyen összetéveszhető a dictionary adatszerkezettel (magyarul hash tábla), de a mélytanulás terminológia során a modell szókészletét, szókincset jelenti."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Nb4J8ttimpEe"
      },
      "outputs": [],
      "source": [
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        super().__init__()\n",
        "\n",
        "        self.emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.emb(x)\n",
        "        return x * math.sqrt(self.d_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxJqRiM_JjO6"
      },
      "source": [
        "# Pozicionális kódolás"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w31C6OqqJkKv"
      },
      "source": [
        "A transzformer architektúra explicit módon nem tudja megragadni a tokenek sorrendjét a szekvenciában, mivel a kontextust csak a figyelem mechanizmuson keresztül modellezi, ami nem veszi figyelembe a szavak sorrendjét. Ezért, a sorrend megragadására a transzformer egy pozicionális kódolás nevű technikát alkalmaz, amely során a szekvencia elemei, a tokenek relatív vagy abszolút pozíciójáról kapott információt beépíti a szekvenciába.\n",
        "\n",
        "Ez azt jelenti, hogy a pozícióra vonatkozó infromációt adnak hozzá a bemeneti beágyazásokhoz a kódoló és dekódoló komponensekben, és ugyanaolyan $d$ dimenzióval rendelkezik, mint a beágyazási réteg, az összeadás művelete érdekében. A kódoló és dekódoló bemenete a $t$ pozícióban levő szóbeágyazás és a $t$ pozícióhoz tartozó pozicionális beágyazás összege. Többféle tanítható és rögzített pozíció kódolás létezik, de a szerzők egy rögzített pozíció kódolást alkalmaztak, amely a következőképpen számolható:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqvtn2ahJpRW"
      },
      "source": [
        "$$\n",
        "\\begin{aligned}\n",
        "P E_{(t, 2 i)} & =\\sin \\left(t / 10000^{2 i / d}\\right) \\\\\n",
        "P E_{(t, 2 i+1)} & =\\cos \\left(t / 10000^{2 i / d}\\right)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "ahol $t$ a pozíció, $i$ a dimenzió indexe és $d$ a modell dimenziója. A pozícionális kódolás minden egyes dimenziója egy szinuszoid függvény. A hullámhosszúságok $2 \\pi$-től $10000 \\cdot 2 \\pi$-ig terjednek, és a dimenziók páros és páratlan indexei szinusz és koszinusz függvények."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "JjvO8qNZmnMr"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, config: ModelConfig) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        pe = torch.zeros(config.max_len, config.d_model)\n",
        "\n",
        "        position = torch.arange(0, config.max_len).unsqueeze(1)\n",
        "        div_term = 1 / 10000.0 ** (torch.arange(0, config.d_model, 2) / config.d_model)\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0)\n",
        "\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-k0ElWTBJ7vH"
      },
      "source": [
        "# Kimeneti réteg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ml1r1Om1J4lN"
      },
      "source": [
        "A modell kimeneti rétege egy lineáris rétegből és egy softmax függvényből áll, amely a dekódoló utolsó rétegének kimenetét egy diszkrét valószínűségi eloszlássá alakítja, amely megadja a következő token valószínűségét a szótár minden elemére vonatkozóan.\n",
        "\n",
        "A két beágyazási réteg súlya össze van kapcsolva egymással és a lineáris rétegben használt súllyal, amit súlykötésnek (weight tying) neveznek. A technika javítja a nyelvi modellek teljesítményét és jelentősen csökkenti a szükséges paraméterek számát.\n",
        "\n",
        "A beágyazás rétegek megtanulják a szavak reprezentációját, így a hasonló jelentésű tokenek vektorai közel helyezkednek el egymáshoz.\n",
        "\n",
        "Press és Wolf kimutatták, hogy az utolsó lineáris réteg súlymátrixa, amelyben minden tokennnek van egy vektor reprezentációja, szintén megjeleníti ezt a tulajdonságot. Ezért javasolták a beágyazási réteg és az utolsó lineáris réteg mátrixainak megosztását, amelyet ma szinte minden nyelvi modell alkalmaz.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46S4tY-FKUCy"
      },
      "source": [
        "A kimeneti réteg a `Transformer` osztály kódját leíró cella 14. sorában található."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad_uK2nwKC6_"
      },
      "source": [
        "# Előrecsatolt hálózat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcN7fQneKD-n"
      },
      "source": [
        "A kódoló és a dekódoló minden rétege tartalmaz egy teljesen összekapcsolt előrecsatolt hálózatot, amelyet minden pozícióra külön-külön, de azonos módon alkalmaz. A hálózat két lineáris leképezésből áll, amelyek között egy ReLU aktivációs függvény helyezkedik el:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\operatorname{FFN}(x) = \\max \\left(0,\\, \\mathbf{x} \\mathbf{W}_1 + \\mathbf{b}_1 \\right) \\mathbf{W}_2 + \\mathbf{b}_2\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "ahol $\\mathbf{W}_1 \\in \\mathbb{R}^{d \\times d_{ff} }, \\mathbf{W}_2 \\in \\mathbb{R}^{d_{ff} \\times d }$ a súlymátrixok, $\\mathbf{b}_1 \\in \\mathbb{R}^{d_{ff}}, \\mathbf{b}_2 \\in \\mathbb{R}^{d}$ az eltolásvektorok és $\\max (0, \\cdot)$ a ReLU aktivációs függvény.\n",
        "\n",
        "A lineáris leképezések minden pozíció esetében azonosak, ugyanakkor a különböző rétegek eltérő paramétereket használnak. Másképpen megfogalmazva, ez a szerkezet két, egydimenziós konvolúciónak is tekinthető, ahol a kernel méret 1. A bemenet és a kimenet dimenziója $d = 512$, míg a belső réteg dimenziója $d_{ff} = 2048$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Rp6CUyaFmqUB"
      },
      "outputs": [],
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(config.d_model, 4 * config.d_model),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * config.d_model, config.d_model),\n",
        "            nn.Dropout(config.mlp_dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.ff(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9deRgJRGKdGT"
      },
      "source": [
        "# Figyelem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCGoHR9rKaER"
      },
      "source": [
        "A figyelem mechanizmus lehetővé teszi a modell számára, hogy a rejtett állapotokból (hidden state) álló szekvencia minden pozíciója kölcsönhatásba lépjen ugyanabban a szekvenciában lévő többi pozícióval, ezáltal az egyes szekvenciákon belül képes megragadni a távolsági (és közeli) kontextust is.\n",
        "\n",
        "Legyen $d$ beágyazás dimenzió és $\\mathbf{h}_t \\in \\mathbb{R}^d$ a bemeneti szekvencia $t$-edik elemének beágyazása egy figyelem rétegnél. A figyelem mechanizmus a bemenetet először három eltérő súlyvektor segítségével három különböző reprezentációra, ún. lekérdezés-, kulcs- és értékvektorokra képezi le, de a gyakorlatban a számításokat mátrixokba rendezik a hatékonyság érdekében. A $\\mathbf{W}^Q, \\mathbf{W}^K \\in \\mathbb{R}^{d \\times d_k}$, $\\mathbf{W}^V \\in \\mathbb{R}^{d \\times d_v}$ súlymátrixok állítják elő a $\\mathbf{q}_t, \\mathbf{k}_t \\in \\mathbb{R}^{d_k}$ és $\\mathbf{v}_t \\in \\mathbb{R}^{d_v}$ reprezentációkat:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "& \\mathbf{q}_t=\\mathbf{W}^Q \\mathbf{h}_t, \\\\\n",
        "& \\mathbf{k}_t=\\mathbf{W}^K \\mathbf{h}_t, \\\\\n",
        "& \\mathbf{v}_t=\\mathbf{W}^V \\mathbf{h}_t,\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "A $\\mathbf{q}$ lekérdezés, ahonnan a figyelem irányul, mint a cél a figyelem mechanizmusban. A $\\mathbf{k}$ kulcs, amire a figyelem irányul, mint a forrás az egyszerű figyelem mechanizmus esetén. A $\\mathbf{v}$ érték az éppen generált kontextus. A képletben szereplő $q,k, v$ betűk az angol query, key, value szavak kezdőbetűi.\n",
        "\n",
        "Következőnek, a lekérdezés- és kulcs skalárszorzatát veszik és elosztják a $\\sqrt{d_k}$ skálázási faktorral a numerikus stabilitás érdekében. Ezután, softmax függvényt alkalmazva a figyelem súlyokat kapják meg, amik az értékkel kerülnek súlyozásra. Ezáltal a $t$-edik token reprezentációjának (vagy kódolt kimenetének) eredménye kiszámítható a figyelem mechanizmus alkalmazásával az alábbi módon:\n",
        "\n",
        "$$\n",
        "\\mathbf{o}_{t}= \\operatorname{softmax} \\left( \\frac{\\mathbf{q}_{t} \\mathbf{k}_{t}^T}{\\sqrt{d_k}}\\right) \\mathbf{v}_{t}\n",
        "$$\n",
        "\n",
        "A szekvencia tokenjeinek kódolt kimenetei egyszerre számíthatók, mivel a fenti egyenletek kifejezhetőek olyan mátrixműveletekkel, amelyek hatékony módon számíthatóak párhuzamos számításokra specializált, modern hardvereken. A fenti egyenletet \\textbf{skálázott skalárszorzat-alapú figyelem}nek (scaled dot-product attention) nevezzük.\n",
        "\n",
        "Megjegyzés. A $d$, $d_k$ és $d_v$ gyakran megegyezik."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "oTxLn8fJmsM_"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    def __init__(self, d_model: int, d_out: int, attn_dropout: float = 0.1, qkv_bias: bool = False) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.key = nn.Linear(d_model, d_out, bias = qkv_bias)\n",
        "        self.query = nn.Linear(d_model, d_out, bias = qkv_bias)\n",
        "        self.value = nn.Linear(d_model, d_out, bias = qkv_bias)\n",
        "\n",
        "        self.dropout = nn.Dropout(attn_dropout)\n",
        "\n",
        "    def forward(self, query, key, value, attn_mask = None, padding_mask = None) -> torch.Tensor:\n",
        "\n",
        "        b, t, d_model = query.shape\n",
        "\n",
        "        q = self.query(query)   # (b, t, d_k)\n",
        "        k = self.key(key)       # (b, t, d_k)\n",
        "        v = self.value(value)   # (b, t, d_v)\n",
        "\n",
        "        # (b, t, t) = (b, t, d) @ (b, d, t)\n",
        "        attn_scores = q @ k.transpose(1, 2)\n",
        "\n",
        "        # figyelem és padding maszk összefűzése\n",
        "        if padding_mask is not None:\n",
        "            padding_mask = padding_mask.view(b, 1, t)\n",
        "            if attn_mask is None:\n",
        "                attn_mask = padding_mask\n",
        "            else:\n",
        "                attn_mask = attn_mask + padding_mask # := logikai és\n",
        "\n",
        "        if attn_mask is not None:\n",
        "            attn_scores.masked_fill_(attn_mask, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / k.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "        context_vec = attn_weights @ v\n",
        "        return context_vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuQrpHnVLEjz"
      },
      "source": [
        "# Többfejű figyelem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1cjJz8hLEmG"
      },
      "source": [
        "A figyelem függvény egyszeri alkalamzása helyett a szerzők előnyösnek találták, hogy a lekérdezéseket, kulcsokat és értékeket $h$ alkalommal, különböző tanulható lineáris leképezésekkel vetítsék le rendre $d_k$ és $d_v$ dimenziókra. Az így kapott projektált lekérdezések, kulcsok és értékek mindegyikén párhuzamosan kerül kiszámításra a figyelemfüggvény, amely \\(d_v\\) dimenziós kimeneteket eredményez. A kimeneteket konkatenálják és egy utolsó lineáris leképezést alkalmaznak, hogy az eredeti $d$ dimenziós kimenet álljon elő. Ezt a technikát többszörös vagy \\textbf{többfejű figyelem}nek (multi-head attention) nevezzük."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lww1518DLNRY"
      },
      "source": [
        "\n",
        "A többfejű figyelem lehetővé teszi, hogy a modell különböző pozíciókban a reprezentáció különböző altereiből származó információkra egyidejűleg \"figyeljen\", gazdagabb kontextuális információt megragadva. Egyetlen figyelemfej esetén az átlagolás ezt a képességet korlátozná, mivel az információk összemosódnak."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvCRmi2mLNUQ"
      },
      "source": [
        "Formálisan, legyen $h$ a figyelemfejek száma és $d_h$ a figyelemfejek dimenziója. A fentebbi egyenletben szereplő tagok a következőképpen alakulnak: a $\\mathbf{q}_t, \\mathbf{k}_t, \\mathbf{v}_t \\in \\mathbb{R}^{d_h h}$ reprezentációk előállnak a $\\mathbf{W}^Q, \\mathbf{W}^K, \\mathbf{W}^V \\in \\mathbb{R}^{d_h h \\times d}$ súlymátrixokból. Azután, a $\\mathbf{q}_t, \\mathbf{k}_t, \\mathbf{v}_t$ vektorokat $h$ részre osztjuk a többfejű figyelem kiszámításához, minden egyes figyelemfejhez egy-egy lekérdezés, kulcs és érték tartozik. Minden figyelemfejre külön-külön alkalmazzuk a skálázott skalárszorzat-alapú figyelem függvényt, majd a kimeneteket konkatenáljuk és egy utolsó lineáris rétegen keresztül leképezzük az eredeti $d$ dimenzióra:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RosYFVAYLk6R"
      },
      "source": [
        "$$\n",
        "\\begin{aligned}\n",
        "& {\\left[\\mathbf{q}_{t, 1} ; \\mathbf{q}_{t, 2} ; \\ldots ; \\mathbf{q}_{t, h}\\right]=\\mathbf{q}_t,} \\\\\n",
        "& {\\left[\\mathbf{k}_{t, 1} ; \\mathbf{k}_{t, 2} ; \\ldots ; \\mathbf{k}_{t, h}\\right]=\\mathbf{k}_t,} \\\\\n",
        "& {\\left[\\mathbf{v}_{t, 1} ; \\mathbf{v}_{t, 2} ; \\ldots ; \\mathbf{v}_{t, h}\\right]=\\mathbf{v}_t,}\n",
        "\\end{aligned}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrsVGeb1LQLW"
      },
      "source": [
        "$$\n",
        "\\mathbf{o}_{t, i}=\\sum_{j=1}^t \\operatorname{Softmax}_j\\left(\\frac{\\mathbf{q}_{t, i}^{\\top} \\mathbf{k}_{j, i}}{\\sqrt{d_h}}\\right) \\mathbf{v}_{j, i},\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathbf{u}_t=\\mathbf{W}^O\\left[\\mathbf{o}_{t, 1} ; \\mathbf{o}_{t, 2} ; \\ldots ; \\mathbf{o}_{t, h}\\right].\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YQ1visILo5H"
      },
      "source": [
        "A szerzők a tanulmányban $h = 8$ párhuzamos figyelemréteget, azaz fejet alkalmaztak. Mindegyik fej esetében $d_k = d_v = d / h = 64$ dimenziót használtak. Mivel minden fej csökkentett dimenziójú térben működik, a teljes számítási költség megközelítőleg megegyezik az egyetlen fejjel végzett, teljes dimenziójú figyelem számítási költségével."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64tqo6h3Lo7s"
      },
      "source": [
        "A transzformer architektúra *háromféle módon alkalmazza a többfejű figyelmet*.\n",
        "\n",
        "Először, a \"kódoló-dekódoló figyelem\" rétegekben a lekérdezés a dekóder előző rétegéből, míg a kulcs és az érték a kódoló kimenetéből származik. Ez lehetővé teszi, hogy a dekódoló minden pozíciója a bemeneti szekvencia összes pozícióját számításba vegye.\n",
        "\n",
        "Másodszor, a kódolóban a figyelem rétegekben a kulcsok, az értékek és a lekérdezések mind ugyanabból a forrásból származnak, azaz a kódoló előző rétegének kimenetéből. Ezt **önfigyelem**nek (self-attention) nevezzük. Ezáltal a kódoló minden pozíciója képes a kódoló előző rétegének bármely pozíciójára figyelni.\n",
        "\n",
        "Harmadszor, a dekódoló önfigyelem rétegei lehetővé teszik, hogy a dekódolóban minden pozíció észrevegye a korábbi, illetve az aktuális pozícióit. Azonban ebben az esetben meg kell akadályozni az információ balról jobbra történő áramlását, hogy az adott token ne függjön az utána következő tokenektől és csak az előtte lévő tokeneket tudja számításba venni, ezáltal megőrizve az autoregresszív tulajdonságot. A skálázott skalárszorzat-alapú figyelem során ez úgy kerül megvalósításra, hogy softmax függvény bemeneténél a jövőbeli pozícióknak megfelelő értékeket $-\\infty$-re állítjuk, aminek hatására a softmax kimenetében ezek nulla értéket vesznek fel, és így nem befolyásolják figyelem számítást. Ezt a módszert **maszkolás**nak (masking), illetve **kauzális maszk**nak (causal mask) is szokták nevezni és kritikus a nyelvi modellek esetében ahol a következő tokent a korábbi tokenek segítségével generáljuk. Az önfigyelem mechanizmus kauzális maszkolással kiegészített változatát **kauzális figyelem**nek (causal attention) is nevezik."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Ze14hwtzmtVN"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model: int, h: int, attn_dropout: float = 0.1, qkv_bias: bool = False):\n",
        "        super().__init__()\n",
        "\n",
        "        assert d_model % h == 0, \"d_model is indivisible by h (n_heads)\"\n",
        "\n",
        "        self.d_out = d_model // h # = d_k = d_v\n",
        "\n",
        "        self.heads = nn.ModuleList(\n",
        "            [ Head(d_model, self.d_out, attn_dropout, qkv_bias) for _ in range(h) ]\n",
        "        )\n",
        "        self.w_o = nn.Linear(h * self.d_out, d_model) # out_proj\n",
        "\n",
        "    def forward(self, query, key, value, attn_mask = None, padding_mask = None):\n",
        "        out = torch.cat([head(query, key, value, attn_mask, padding_mask) for head in self.heads], dim=-1)\n",
        "        out = self.w_o(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "QxEd0NOk37xb"
      },
      "outputs": [],
      "source": [
        "def create_mask(src, tgt, padding_token: int = 1024) -> tuple:\n",
        "\n",
        "    device = src.device\n",
        "    tgt_seq_len = tgt.shape[1]\n",
        "\n",
        "    # kauzális maszk, ebben a lépésben még nem állítjuk\n",
        "    # -inf-re az értékek, csak a figyelem osztály metódusában\n",
        "    tgt_mask = (torch.triu(torch.ones((tgt_seq_len, tgt_seq_len), device=device)) == 0).transpose(0,1)\n",
        "\n",
        "    # padding maszk, ahol az igaz érték := maszkolás\n",
        "    src_padding_mask = (src == padding_token)\n",
        "    tgt_padding_mask = (tgt == padding_token)\n",
        "\n",
        "    return tgt_mask, src_padding_mask, tgt_padding_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYs5A4EeOLd-"
      },
      "source": [
        "# Futtatás"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "6l8_sFhKm-dV"
      },
      "outputs": [],
      "source": [
        "config = ModelConfig()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "bm2t1u4GzOLd"
      },
      "outputs": [],
      "source": [
        "model = Transformer(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "l88UpxYJO-4z"
      },
      "outputs": [],
      "source": [
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FG6ev7EBN3Qi",
        "outputId": "0e8d8645-d8a9-4c85-f0af-8270c8a4edc2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "===========================================================================\n",
              "Layer (type:depth-idx)                             Param #\n",
              "===========================================================================\n",
              "Transformer                                        --\n",
              "├─Embeddings: 1-1                                  --\n",
              "│    └─Embedding: 2-1                              524,288\n",
              "├─Embeddings: 1-2                                  --\n",
              "│    └─Embedding: 2-2                              524,288\n",
              "├─PositionalEncoding: 1-3                          --\n",
              "├─Encoder: 1-4                                     --\n",
              "│    └─Sequential: 2-3                             --\n",
              "│    │    └─EncoderBlock: 3-1                      3,150,848\n",
              "│    │    └─EncoderBlock: 3-2                      3,150,848\n",
              "│    │    └─EncoderBlock: 3-3                      3,150,848\n",
              "│    │    └─EncoderBlock: 3-4                      3,150,848\n",
              "│    │    └─EncoderBlock: 3-5                      3,150,848\n",
              "│    │    └─EncoderBlock: 3-6                      3,150,848\n",
              "│    │    └─EncoderBlock: 3-7                      3,150,848\n",
              "│    │    └─EncoderBlock: 3-8                      3,150,848\n",
              "├─Decoder: 1-5                                     --\n",
              "│    └─Sequential: 2-4                             --\n",
              "│    │    └─DecoderBlock: 3-9                      4,200,960\n",
              "│    │    └─DecoderBlock: 3-10                     4,200,960\n",
              "│    │    └─DecoderBlock: 3-11                     4,200,960\n",
              "│    │    └─DecoderBlock: 3-12                     4,200,960\n",
              "│    │    └─DecoderBlock: 3-13                     4,200,960\n",
              "│    │    └─DecoderBlock: 3-14                     4,200,960\n",
              "│    │    └─DecoderBlock: 3-15                     4,200,960\n",
              "│    │    └─DecoderBlock: 3-16                     4,200,960\n",
              "├─Linear: 1-6                                      525,312\n",
              "===========================================================================\n",
              "Total params: 60,388,352\n",
              "Trainable params: 60,388,352\n",
              "Non-trainable params: 0\n",
              "==========================================================================="
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summary(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATSrrbbyRbqm"
      },
      "source": [
        "A teszteléshez hozzunk létre egy forrás szekvencia és cél szekvencia tenzort, aminek mérete a batch-méret és szekvencia hossza.\n",
        "\n",
        "Természetesen, a forrásnyelv szövege nem feltétlen egyenlő a célnyelv szövegével, ezért ún. padding-et alkalmazunk, aminek lényege, hogy egy (padding) tokent adunk hozzá a szekvenciához, hogy két tenzor mérete megegezzen.\n",
        "\n",
        "A padding tokenek nem kerülnek bele a figyelem számításába.\n",
        "\n",
        "Az egyszerűség kedvéért tegyük fel, hogy a forrás és cél szekvenciának 10 eleme (tokenje) van és a batch-méret 2.\n",
        "\n",
        "A képzeletbeli szótár (`vocab_size`) jelen esetben 1024 szavas. Továbbá, legyen a padding token értéke 1024."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "vg4XpI-4qVAm"
      },
      "outputs": [],
      "source": [
        "batch_size = 2\n",
        "vocab_size = 1024\n",
        "seq_len = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "kT9UwOphtJlA"
      },
      "outputs": [],
      "source": [
        "src = torch.randint(0, vocab_size, (batch_size, seq_len), device=device)\n",
        "tgt = torch.randint(0, vocab_size, (batch_size, seq_len), device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "yRSghuReOkNO"
      },
      "outputs": [],
      "source": [
        "assert src.shape == tgt.shape, \"source and target tensors are not equal\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Lwi_HX04SA_Y"
      },
      "outputs": [],
      "source": [
        "results = model(src, tgt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "dXmdUVvCmRm8"
      },
      "outputs": [],
      "source": [
        "assert results.shape == (batch_size, seq_len, vocab_size), \"results tensor shape is not correct\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhieJxYhqT91",
        "outputId": "71488af0-beab-4cba-8427-2f0fb3eb2623"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[-0.7474,  0.6309,  1.3059,  ..., -0.3122, -0.5644,  1.3268],\n",
              "         [-0.4822,  0.6127,  0.3049,  ..., -0.3428, -0.3646,  0.7711],\n",
              "         [ 0.5773,  0.5921,  1.2765,  ..., -0.0479, -0.2274,  1.0196],\n",
              "         ...,\n",
              "         [ 0.3785,  0.7820,  0.2595,  ..., -0.3524, -0.4440,  1.5753],\n",
              "         [-0.0929,  0.9529,  1.1653,  ...,  0.1805, -0.0149,  0.5563],\n",
              "         [ 0.1143,  0.2492,  0.3625,  ..., -0.4894,  0.1405,  0.7082]],\n",
              "\n",
              "        [[ 0.0780,  0.2454,  0.6327,  ..., -0.4938, -0.6576,  1.0808],\n",
              "         [-0.3486, -0.2542,  0.8601,  ..., -0.4373, -0.4138,  0.4963],\n",
              "         [-0.2479, -0.0816,  1.2530,  ..., -0.1385, -0.5231,  0.9671],\n",
              "         ...,\n",
              "         [-0.3881,  0.3099, -0.0347,  ..., -0.3726,  0.4479,  1.3677],\n",
              "         [-0.2242,  0.1946,  0.5817,  ..., -0.2176,  0.1519,  0.4154],\n",
              "         [-0.1084,  0.3643,  0.6808,  ...,  0.2020, -0.2341,  0.9754]]],\n",
              "       device='cuda:0', grad_fn=<ViewBackward0>)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTLQOEA-rr2y"
      },
      "source": [
        "Az eredmény egy tenzor, aminek mérete a batch-méret, szekvencia hossza és a szekvencia minden eleméhez egy valószínűségi eloszlás (am), ami megadja az adott pozícióra prediktált következő token teljes szókészletre kiterjedő valószínűségeit (logitjeit)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
