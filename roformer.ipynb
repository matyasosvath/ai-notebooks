{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RoFormer: Enhanced Transformer with Rotary Position Embedding.\n",
        "\n",
        "Su, Jianlin, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. \"RoFormer: Enhanced Transformer with Rotary Position Embedding.\" arXiv:2104.09864. Preprint, arXiv, November 8, 2023. https://doi.org/10.48550/arXiv.2104.09864.\n"
      ],
      "metadata": {
        "id": "Rx-_cfj4gG58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "id": "W4hpMqDKEUk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "1wvwGgps_Ehv"
      },
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from torchinfo import summary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "3oiukfvzxX9R"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "id": "P3CX55jxxZYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class ModelArgs:\n",
        "    block_size: int = 1024              # seq_len\n",
        "    vocab_size: int = 32_000\n",
        "    hidden_size: int = 768              # embedding_size\n",
        "    type_vocab_size: int = 2\n",
        "    intermediate_size: int = 3072       # 4 x hidden size\n",
        "    num_attention_heads: int = 8\n",
        "    n_layers: int = 4\n",
        "    n_heads: int = 4\n",
        "    dim: int = 1024\n",
        "    head_dim: int = 64\n",
        "    rope_base: float = 100_0000.0\n",
        "    norm_eps: float = 1e-5\n",
        "    dropout: float = 0.1\n",
        "    attn_dropout: float = 0.1\n",
        "    layer_norm_eps: float = 1e-12"
      ],
      "metadata": {
        "id": "61Pb6Mi-_Idw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = ModelArgs()"
      ],
      "metadata": {
        "id": "f39VbD7CxG63"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.c_fc = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        self.c_proj = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.act = nn.GELU()\n",
        "\n",
        "        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        hidden_states = x\n",
        "        x = self.c_fc(x)\n",
        "        x = self.act(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.layer_norm(x + hidden_states)\n",
        "        return x"
      ],
      "metadata": {
        "id": "4j0MnCT5lWiz"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A (szög)frekvenciák listája:\n",
        "\n",
        "$$\n",
        "\\omega_k =\\frac{1}{\\operatorname{base}^{2 k / d}}, \\quad k=0,1, \\ldots, d / 2-1\n",
        "$$\n"
      ],
      "metadata": {
        "id": "TyulJt7LdfFJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pozíciók $t$ és a frekvenciák $\\omega_k$ külső szorzata adja a szögeket:\n",
        "\n",
        "$$\n",
        "\\theta_{t, k} = t \\otimes \\omega_k  = t \\cdot \\omega_k^T =\n",
        "\\left[\n",
        "    \\begin{array}{cccc}\n",
        "        t_1 \\omega_{k1} & t_1 \\omega_{k2} & \\ldots &  t_1 \\omega_{kn} \\\\\n",
        "        t_2 \\omega_{k1} & t_2 \\omega_{k2} & \\ldots & t_2 \\omega_{kn} \\\\\n",
        "        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "        t_m \\omega_{k1} & t_m \\omega_{k2} & \\ldots & t_m \\omega_{kn}\n",
        "    \\end{array}\n",
        "\\right] \\quad \\in \\mathbb{R}^{m \\times(d / 2)}\n",
        "$$\n",
        "\n",
        "ahol $t \\in \\mathbb{R}^m $ és $ \\omega \\in \\mathbb{R}^{d / 2} $.\n",
        "\n",
        "Ezek lesznek a szögek (radiánban), amivel a $ ( x_{2k}, x_{2k+1} ) $ dimenziópárt később elforgatjuk. Azaz a mátrix minden sora egy pozíció $t$, minden oszlopa egy dimenziópár $k$ szöge radiánban."
      ],
      "metadata": {
        "id": "H8X5wtWyc1_O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A komplex szám trigonometrikus alakja:\n",
        "$$\n",
        "z=|z|(\\cos \\theta + i \\sin \\theta)\n",
        "$$\n",
        "\n",
        "ahol $|z|$ a komplex szám abszolút értéke (hossza) és ha $z$ az egységsugarú körön van, akkor $|z|=1$. Tehát:\n",
        "\n",
        "$$z=\\cos \\theta+i \\sin \\theta$$\n",
        "\n",
        "Egy komplex számokból (trigonometrikus alak) álló tenzor kerül létrehozásra, amielynek valós része a $\\cos \\theta$ és imaginárius része a $\\sin \\theta$. A gyakorlatban a számitáshoz ezt két valós komponensre bontjuk, hogy valós műveletekkel épitsük fel a forgatást.\""
      ],
      "metadata": {
        "id": "aVKW98iYkAMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def precompute_freqs_cis(\n",
        "        seq_len: int,\n",
        "        head_dim: int,\n",
        "        base: int = 10_000\n",
        ") -> torch.Tensor:\n",
        "\n",
        "    freqs = 1.0 / (base ** (torch.arange(0, head_dim, 2)[: (head_dim // 2)].float() / head_dim)) # (d // 2)\n",
        "\n",
        "    t = torch.arange(seq_len, device=freqs.device) # [0, 1, 2, ..., seq_len-1], (seql_len)\n",
        "\n",
        "    freqs = torch.outer(t, freqs) # (seq_len, d // 2)\n",
        "\n",
        "    # x' = x * cos(angle) + x * sin(angle) * i, where i is imaginary number\n",
        "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs) # (seq_len, d // 2)\n",
        "\n",
        "    cache = torch.stack([freqs_cis.real, freqs_cis.imag], dim=-1) # (seq_len, d // 2, 2)\n",
        "\n",
        "    return cache.to(dtype=torch.bfloat16)"
      ],
      "metadata": {
        "id": "2xAGf2IDBh8E"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A rotációs mátrix ($R$), olyan transzformációs mátrix, amely euklideszi térben forgatás végrehajtására szolgál.\n",
        "\n",
        "$$\n",
        "R=\\left[\\begin{array}{cc}\n",
        "\\cos \\theta & -\\sin \\theta \\\\\n",
        "\\sin \\theta & \\cos \\theta\n",
        "\\end{array}\\right]\n",
        "$$"
      ],
      "metadata": {
        "id": "4bzH7gNqpQpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_rotary_emb(x: Tensor, freqs_cis: Tensor) -> torch.Tensor:\n",
        "    xshaped = x.float().reshape(*x.shape[:-1], -1, 2)\n",
        "    freqs_cis = freqs_cis.view(1, xshaped.size(1), 1, xshaped.size(3), 2)\n",
        "    x_out = torch.stack(\n",
        "        [\n",
        "            xshaped[..., 0] * freqs_cis[..., 0] - xshaped[..., 1] * freqs_cis[..., 1],\n",
        "            xshaped[..., 1] * freqs_cis[..., 0] + xshaped[..., 0] * freqs_cis[..., 1],\n",
        "        ],\n",
        "        -1,\n",
        "    )\n",
        "    x_out = x_out.flatten(3)\n",
        "    return x_out.type_as(x)"
      ],
      "metadata": {
        "id": "oqanTX4Ne1Uh"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        assert config.hidden_size % config.n_heads == 0, \"hidden size is indivisible by n_heads (number of attention heads)\"\n",
        "\n",
        "        self.n_heads = config.n_heads                               # h\n",
        "        self.head_dim = config.hidden_size // config.n_heads         # d_h\n",
        "        self.all_head_size = config.n_heads * self.head_dim\n",
        "\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.attn_dropout)\n",
        "\n",
        "        self.w_o = nn.Linear(config.hidden_size, config.hidden_size) # (d, d)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, freqs_cis: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "\n",
        "        batch_size, seq_len, hidden_size = x.shape # (b,t,d)\n",
        "\n",
        "        q = self.query(x).view(batch_size, seq_len, self.n_heads, self.head_dim)\n",
        "        k = self.key(x).view(batch_size, seq_len, self.n_heads, self.head_dim)\n",
        "        v = self.value(x).view(batch_size, seq_len, self.n_heads, self.head_dim)\n",
        "\n",
        "        q = apply_rotary_emb(q, freqs_cis)\n",
        "        k = apply_rotary_emb(k, freqs_cis)\n",
        "\n",
        "        # (b, h, t, d_h) <- (b, t, h, d_h)\n",
        "        q, k, v = map(lambda x: x.transpose(1, 2), (q, k, v))\n",
        "\n",
        "        # (b, h, t, t) = (b, h, t, d_h) @ (b, h, d_h, t)\n",
        "        attn_scores = torch.matmul(q, k.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            attn_scores = attn_scores + attention_mask\n",
        "\n",
        "        attention_probs = F.softmax(attn_scores, dim=-1)\n",
        "\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        # (b, h, t, d_v) = (b, h, t, t) @ (b, h, t, d_v)\n",
        "        # (b, t, h, d_v) <- (b, h, t, d_v)\n",
        "        context = torch.matmul(attention_probs, v).transpose(1, 2).contiguous()\n",
        "\n",
        "        context = context.view(batch_size, seq_len, hidden_size)\n",
        "\n",
        "        return self.w_o(context)\n"
      ],
      "metadata": {
        "id": "FqQWzIElmnO0"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadAttention(config)\n",
        "        self.feed_forward = FeedForward(config)\n",
        "        self.ffn_norm = nn.LayerNorm(config.hidden_size, config.layer_norm_eps)\n",
        "        self.attention_norm = nn.LayerNorm(config.hidden_size, config.layer_norm_eps)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, freqs_cis: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        h = x + self.attention(self.attention_norm(x), freqs_cis, attention_mask)\n",
        "        out = h + self.feed_forward(self.ffn_norm(h))\n",
        "        return out"
      ],
      "metadata": {
        "id": "YDJ2kn59iev0"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RoFormer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.token_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
        "        self.layers = nn.ModuleList([EncoderLayer(config) for i in range(config.n_layers)])\n",
        "        self.output = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
        "\n",
        "        self.freqs_cis = precompute_freqs_cis(config.block_size, config.hidden_size // config.n_heads, config.rope_base)\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            x: torch.Tensor,\n",
        "            attention_mask: Optional[torch.Tensor] = None,\n",
        "            input_pos: Optional[torch.Tensor] = None # (0, seq_len-1)\n",
        "    ) -> torch.Tensor:\n",
        "\n",
        "        batch_size, seq_length = x.size()\n",
        "        freqs_cis = self.freqs_cis[input_pos]\n",
        "\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones(((batch_size, seq_length)), device=x.device)\n",
        "        attention_mask = self.get_extended_attention_mask(attention_mask, x.shape)\n",
        "\n",
        "        x = self.token_embeddings(x)\n",
        "\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = layer(x, freqs_cis, attention_mask)\n",
        "        logits = self.output(x)\n",
        "        return logits\n",
        "\n",
        "    def get_extended_attention_mask(self, attention_mask: torch.Tensor, input_shape) -> torch.Tensor:\n",
        "        # attention_mask is 1.0 for positions we want to attend and 0.0 for masked positions\n",
        "        # create attention mask of shape (batch_size, 1, 1, seq_len), where 1 will be 0 and\n",
        "        # 0 will the smallest value for a given dtype (-inf)\n",
        "        extended_attention_mask = attention_mask[:, None, None, :] #\n",
        "        extended_attention_mask = extended_attention_mask.to(dtype=attention_mask.dtype)\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * torch.finfo(attention_mask.dtype).min\n",
        "        return extended_attention_mask"
      ],
      "metadata": {
        "id": "B_ZpxjB0ie6S"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "roformer = RoFormer(config).to(device)"
      ],
      "metadata": {
        "id": "CIvKAOcMj85J"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randint(0, config.vocab_size, (4, config.block_size), device=device).to(torch.long)"
      ],
      "metadata": {
        "id": "3ZPkoZeIMZ88"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(roformer, input_data=x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9VyUK6UMd9n",
        "outputId": "fba88c84-8864-4080-b111-4f7b84134785"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "RoFormer                                 [4, 1024, 32000]          --\n",
              "├─Embedding: 1-1                         [4, 1024, 768]            24,576,000\n",
              "├─ModuleList: 1-2                        --                        --\n",
              "│    └─EncoderLayer: 2-1                 [4, 1024, 768]            --\n",
              "│    │    └─LayerNorm: 3-1               [4, 1024, 768]            1,536\n",
              "│    │    └─MultiHeadAttention: 3-2      [4, 1024, 768]            2,362,368\n",
              "│    │    └─LayerNorm: 3-3               [4, 1024, 768]            1,536\n",
              "│    │    └─FeedForward: 3-4             [4, 1024, 768]            4,723,968\n",
              "│    └─EncoderLayer: 2-2                 [4, 1024, 768]            --\n",
              "│    │    └─LayerNorm: 3-5               [4, 1024, 768]            1,536\n",
              "│    │    └─MultiHeadAttention: 3-6      [4, 1024, 768]            2,362,368\n",
              "│    │    └─LayerNorm: 3-7               [4, 1024, 768]            1,536\n",
              "│    │    └─FeedForward: 3-8             [4, 1024, 768]            4,723,968\n",
              "│    └─EncoderLayer: 2-3                 [4, 1024, 768]            --\n",
              "│    │    └─LayerNorm: 3-9               [4, 1024, 768]            1,536\n",
              "│    │    └─MultiHeadAttention: 3-10     [4, 1024, 768]            2,362,368\n",
              "│    │    └─LayerNorm: 3-11              [4, 1024, 768]            1,536\n",
              "│    │    └─FeedForward: 3-12            [4, 1024, 768]            4,723,968\n",
              "│    └─EncoderLayer: 2-4                 [4, 1024, 768]            --\n",
              "│    │    └─LayerNorm: 3-13              [4, 1024, 768]            1,536\n",
              "│    │    └─MultiHeadAttention: 3-14     [4, 1024, 768]            2,362,368\n",
              "│    │    └─LayerNorm: 3-15              [4, 1024, 768]            1,536\n",
              "│    │    └─FeedForward: 3-16            [4, 1024, 768]            4,723,968\n",
              "├─Linear: 1-3                            [4, 1024, 32000]          24,576,000\n",
              "==========================================================================================\n",
              "Total params: 77,509,632\n",
              "Trainable params: 77,509,632\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 310.04\n",
              "==========================================================================================\n",
              "Input size (MB): 0.03\n",
              "Forward/backward pass size (MB): 2281.70\n",
              "Params size (MB): 310.04\n",
              "Estimated Total Size (MB): 2591.77\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gpKe6m1q38_I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}