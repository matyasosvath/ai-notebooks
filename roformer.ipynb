{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx-_cfj4gG58"
      },
      "source": [
        "# RoFormer: Enhanced Transformer with Rotary Position Embedding.\n",
        "\n",
        "Su, Jianlin, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. \"RoFormer: Enhanced Transformer with Rotary Position Embedding.\" arXiv:2104.09864. Preprint, arXiv, November 8, 2023. https://doi.org/10.48550/arXiv.2104.09864.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4hpMqDKEUk1",
        "outputId": "c2ec2a43-bd96-45a7-bc11-68eab090d302"
      },
      "outputs": [],
      "source": [
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "1wvwGgps_Ehv"
      },
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from torchinfo import summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "3oiukfvzxX9R"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3CX55jxxZYz",
        "outputId": "67f5def0-db2f-45ae-b135-3bcba70bd576"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "61Pb6Mi-_Idw"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ModelArgs:\n",
        "    block_size: int = 1024              # seq_len\n",
        "    vocab_size: int = 32_000\n",
        "    hidden_size: int = 768              # embedding_size\n",
        "    type_vocab_size: int = 2\n",
        "    intermediate_size: int = 3072       # 4 x hidden size\n",
        "    num_attention_heads: int = 8\n",
        "    n_layers: int = 4\n",
        "    n_heads: int = 4\n",
        "    dim: int = 1024\n",
        "    head_dim: int = 64\n",
        "    rope_base: float = 100_0000.0\n",
        "    norm_eps: float = 1e-5\n",
        "    dropout: float = 0.1\n",
        "    attn_dropout: float = 0.1\n",
        "    layer_norm_eps: float = 1e-12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "f39VbD7CxG63"
      },
      "outputs": [],
      "source": [
        "config = ModelArgs()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "4j0MnCT5lWiz"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.c_fc = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        self.c_proj = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.act = nn.GELU()\n",
        "\n",
        "        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        hidden_states = x\n",
        "        x = self.c_fc(x)\n",
        "        x = self.act(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.layer_norm(x + hidden_states)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJGDpRjzOeFU"
      },
      "source": [
        "Legyen az $\\mathbb{S}_N=\\left\\{w_i\\right\\}_{i=1}^N$ egy $N$ bemeneti tokenből álló szekvencia, ahol $w_i$ az $i$-edik elem.\n",
        "\n",
        "Az $\\mathbb{S}_N$-hez tartozó (szó)beágyazást jelölje a $\\mathbb{E}_N=\\left\\{\\mathbf{x}_i\\right\\}_{i=1}^N$, ahol $\\mathbf{x}_i \\in \\mathbb{R}^d$ a $w_i$ token $d$ dimenziós (szó)beágyazás vektora *pozicionális információ nélkül*.\n",
        "\n",
        "Az önfigyelem mechanizmus először a szóbeágyazások $q$,$k$, és $v$ reprezentációkká alakítja.\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathbf{q}_m & =f_q\\left(\\mathbf{x}_m, m\\right) = \\mathbf{W}_q \\mathbf{x}_m \\\\\n",
        "\\mathbf{k}_n & =f_k\\left(\\mathbf{x}_n, n\\right) = \\mathbf{W}_k \\mathbf{x}_n \\\\\n",
        "\\mathbf{v}_n & =f_v\\left(\\mathbf{x}_n, n\\right)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "ahol a $\\mathbf{q}_m, \\mathbf{k}_n$ és $\\mathbf{v}_n$ az $m$-edik és $n$-edik pozíciót jelöli.\n",
        "\n",
        "A önfigyelem képletében a $$\\mathbf{q}_m^{\\top} \\mathbf{k}_n$$ rész jelenti a tudás átadását különböző tokenek között a különböző pozíciókban."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_zbHLRnRtGz"
      },
      "source": [
        "A korábbi (abszolút és relatív) pozícionális beágyazások során a figyelem mechanizmus előtt történik meg a művelet, azaz\n",
        "\n",
        "$$\n",
        "f_{t: t \\in\\{q, k, v\\}}\\left(\\mathbf{x}_i, i\\right) := \\mathbf{W}_{t: t \\in\\{q, k, v\\}}\\left(\\mathbf{x}_i+\\mathbf{p}_i\\right),\n",
        "$$\n",
        "\n",
        "ahol $\\mathbf{p}$ adja a pozícionális infromációt és $\\mathbf{p}_i \\in \\mathbb{R}^d$ egy $d$ dimenziós vektor, a az $\\mathbf{x}_i$ token pozíciója."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFfRm7idWzzm"
      },
      "source": [
        "A rotációs pozíció beágyazás (rotary position embedding, röviden RoPE) a vektorok geometriai tulajdonságait a két dimenziós síkon és komplex számként is értelmezhető alakját felhasználva az alábbi megoldást javasolják:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "f_q\\left(\\mathbf{x}_m, m\\right) & =\\left(\\mathbf{W}_q \\mathbf{x}_m\\right) e^{i m \\theta} \\\\\n",
        "f_k\\left(\\mathbf{x}_n, n\\right) & =\\left(\\mathbf{W}_k \\mathbf{x}_n\\right) e^{i n \\theta} \\\\\n",
        "\\end{aligned}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPRDLfpYgpiT"
      },
      "source": [
        "Adott $(x_1, x_2)$ vektort fel lehet írni a $z = x_1 + x_2 i$ komplex számmal (algebrai alak). Tetszőleges $z \\in \\mathbb{C}$ adott $\\theta$ szöggel való forgatását az $z \\times e^{i \\theta}$ adja."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYYTsTI_fkPD"
      },
      "source": [
        "Ha $m \\theta$ a pozíciófüggő (szög)frekvencia, akkor a forgatást a komplex számísíkon a $e^{i m \\theta}$ forgatás jelöli.\n",
        "\n",
        "Az $e^{i m \\theta}$ az Euler-képlet alapján kifejezhető:\n",
        "$$\n",
        "e^{i m \\theta} = \\cos(m \\theta) +i\\sin (m \\theta)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LajOGjvQgfCN"
      },
      "source": [
        "Ezért, tetszőleges $z \\in \\mathbb{C}$ szám forgatása felírható:\n",
        "$$\n",
        "z^{\\prime}=z \\cdot e^{i m \\theta}=\\left(x_1+i x_2\\right)(\\cos (m \\theta)+i \\sin (m \\theta)) .\n",
        "$$\n",
        "\n",
        "$$\n",
        "z^{\\prime}=\\left(x_1 \\cos (m \\theta)-x_2 \\sin (m \\theta)\\right)+i\\left(x_1 \\sin (m \\theta)+x_2 \\cos (m \\theta)\\right)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnsDPoN5xZG-"
      },
      "source": [
        "A RoPE során a $\\cos(m \\theta)$ és $\\sin(m \\theta)$ értékek tárolásra kerülnek egy rotációs mátrixban ($\\mathbf{R}_{\\Theta}$) a\n",
        "$$\n",
        "e^{i m \\theta} = \\cos(m \\theta) +i\\sin (m \\theta)\n",
        "$$\n",
        "\n",
        "alapján, ahol $\\Theta$ a szögfrekvenciák skálája\n",
        "\n",
        "$$\n",
        "\\Theta=\\left\\{\\theta_i=10000^{-2(i-1) / d}, i \\in[1,2, \\ldots, d / 2]\\right\\}.\n",
        "$$\n",
        "\n",
        "Mivel az eredményeket a két dimenziós sík alapján akarták általánosítani bármely $ \\mathbf{x}_i \\in \\mathbb{R}^d$ vektorra, ahol $d$ páros, ezért a dimenziót kettővel osztva, $d/2$ altér lesz.\n",
        "\n",
        "A számítást a `precompute_freqs_cis` függvény valósítja meg."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfLkLrLcoyTH"
      },
      "source": [
        "A $f_q$ és $f_k$ függvények felírhatóak az alábbi alakban:\n",
        "\n",
        "$$\n",
        "f_{\\{q, k\\}}\\left(\\mathbf{x}_m, m\\right) =\n",
        "\\mathbf{R}_{\\Theta, m}^d \\mathbf{W}_{\\{q, k\\}} \\mathbf{x}_m\n",
        "$$\n",
        "\n",
        "A komputációs hatékonyság miatt a $\\mathbf{R}_{\\Theta, m}^d$ és $\\boldsymbol{x} \\in \\mathbb{R}^d$ realizációja a\n",
        "\n",
        "$$\n",
        "\\mathbf{R}_{\\Theta, m}^d \\mathbf{x}=\\left(\\begin{array}{c}\n",
        "x_1 \\\\\n",
        "x_2 \\\\\n",
        "x_3 \\\\\n",
        "x_4 \\\\\n",
        "\\vdots \\\\\n",
        "x_{d-1} \\\\\n",
        "x_d\n",
        "\\end{array}\\right) \\otimes\\left(\\begin{array}{c}\n",
        "\\cos m \\theta_1 \\\\\n",
        "\\cos m \\theta_1 \\\\\n",
        "\\cos m \\theta_2 \\\\\n",
        "\\cos m \\theta_2 \\\\\n",
        "\\vdots \\\\\n",
        "\\cos m \\theta_{d / 2} \\\\\n",
        "\\cos m \\theta_{d / 2}\n",
        "\\end{array}\\right)+\\left(\\begin{array}{c}\n",
        "-x_2 \\\\\n",
        "x_1 \\\\\n",
        "-x_4 \\\\\n",
        "x_3 \\\\\n",
        "\\vdots \\\\\n",
        "-x_d \\\\\n",
        "x_{d-1}\n",
        "\\end{array}\\right) \\otimes\\left(\\begin{array}{c}\n",
        "\\sin m \\theta_1 \\\\\n",
        "\\sin m \\theta_1 \\\\\n",
        "\\sin m \\theta_2 \\\\\n",
        "\\sin m \\theta_2 \\\\\n",
        "\\vdots \\\\\n",
        "\\sin m \\theta_{d / 2} \\\\\n",
        "\\sin m \\theta_{d / 2}\n",
        "\\end{array}\\right)\n",
        "$$\n",
        "\n",
        "amelyet az `apply_rotary_emb` függvény valósít meg."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5s5ZgcSOWz16"
      },
      "source": [
        "Megjegyzések\n",
        "\n",
        "- A rotációs mátrix olyan transzformációs mátrix, amely euklideszi térben forgatás végrehajtására szolgál.\n",
        "\n",
        "- Két complex vektor, $\\mathbf{a}$ és $\\mathbf{b}$ skaláris szorzata (dot product)\n",
        "$$\n",
        "\\mathbf{a} \\cdot \\mathbf{b}=\\sum_i a_i \\overline{b_i},\n",
        "$$\n",
        "\n",
        "- Komplex számokon az összeadás és szorás művelet értelmezése\n",
        "$$\n",
        "\\begin{aligned}\n",
        "&(a, b)+(c, d) \\doteq(a+c, b+d), \\\\\n",
        "&(a, b)(c, d) \\doteq(a c-b d, b c+a d) .\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "- Az Euler-képlet a komplex matematikai analízis egy formulája, mely megmutatja, hogy szoros kapcsolat van a szögfüggvények és a komplex exponenciális függvény között. Az Euler-képlet azt állítja, hogy minden valós $x$ számra igaz (és ahol $e$ az Euler-féle szám, a természetes logaritmus alapja, e=2,718):\n",
        "\n",
        "$$\n",
        "e^{ix} = \\cos(x) +i\\sin (x)\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUNcUxZsnjlc"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "2xAGf2IDBh8E"
      },
      "outputs": [],
      "source": [
        "def precompute_freqs_cis(\n",
        "        seq_len: int,\n",
        "        head_dim: int,\n",
        "        base: int = 10_000\n",
        ") -> torch.Tensor:\n",
        "\n",
        "    freqs = 1.0 / (base ** (torch.arange(0, head_dim, 2)[: (head_dim // 2)].float() / head_dim))    # (d // 2)\n",
        "\n",
        "    t = torch.arange(seq_len, device=freqs.device)                                                 # [0, 1, 2, ..., seq_len-1], (seql_len)\n",
        "\n",
        "    freqs = torch.outer(t, freqs)                                                                  # (seq_len, d // 2)\n",
        "\n",
        "    # x' = x * cos(theta) + x * sin(theta) * i, ahol i az imaginárius szám\n",
        "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)                                         # (seq_len, d // 2)\n",
        "\n",
        "    cache = torch.stack([freqs_cis.real, freqs_cis.imag], dim=-1)                                  # (seq_len, d // 2, 2)\n",
        "\n",
        "    return cache.to(dtype=torch.bfloat16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "oqanTX4Ne1Uh"
      },
      "outputs": [],
      "source": [
        "def apply_rotary_emb(\n",
        "    x: Tensor,                                                                          # (bs, block_size, n_heads, head_dim)\n",
        "    freqs_cis: Tensor                                                                   # (block_size, head_dim // 2, 2)\n",
        ") -> torch.Tensor:\n",
        "    xshaped = x.float().reshape(*x.shape[:-1], -1, 2)                                    # (bs, block_size, n_heads, head_dim // 2, 2)\n",
        "    freqs_cis = freqs_cis.view(1, xshaped.size(1), 1, xshaped.size(3), 2)               # (1, block_size, 1, head_dim // 2, 2)\n",
        "    x_out = torch.stack(\n",
        "        [\n",
        "            # első komponens rotáció: x1 * cos(theta) - x2 * sin(theta)\n",
        "            xshaped[..., 0] * freqs_cis[..., 0] - xshaped[..., 1] * freqs_cis[..., 1],\n",
        "            # második komponens rotáció: x2 * cos(theta) + x1 * sin(theta)\n",
        "            xshaped[..., 1] * freqs_cis[..., 0] + xshaped[..., 0] * freqs_cis[..., 1],\n",
        "        ],\n",
        "        -1,\n",
        "    )\n",
        "    x_out = x_out.flatten(3)\n",
        "    return x_out.type_as(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "FqQWzIElmnO0"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        assert config.hidden_size % config.n_heads == 0, \"hidden size is indivisible by n_heads (number of attention heads)\"\n",
        "\n",
        "        self.n_heads = config.n_heads                               # h\n",
        "        self.head_dim = config.hidden_size // config.n_heads         # d_h\n",
        "        self.all_head_size = config.n_heads * self.head_dim\n",
        "\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.attn_dropout)\n",
        "\n",
        "        self.w_o = nn.Linear(config.hidden_size, config.hidden_size) # (d, d)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, freqs_cis: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "\n",
        "        batch_size, seq_len, hidden_size = x.shape # (b,t,d)\n",
        "\n",
        "        q = self.query(x).view(batch_size, seq_len, self.n_heads, self.head_dim)\n",
        "        k = self.key(x).view(batch_size, seq_len, self.n_heads, self.head_dim)\n",
        "        v = self.value(x).view(batch_size, seq_len, self.n_heads, self.head_dim)\n",
        "\n",
        "        q = apply_rotary_emb(q, freqs_cis)\n",
        "        k = apply_rotary_emb(k, freqs_cis)\n",
        "\n",
        "        # (b, h, t, d_h) <- (b, t, h, d_h)\n",
        "        q, k, v = map(lambda x: x.transpose(1, 2), (q, k, v))\n",
        "\n",
        "        # (b, h, t, t) = (b, h, t, d_h) @ (b, h, d_h, t)\n",
        "        attn_scores = torch.matmul(q, k.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            attn_scores = attn_scores + attention_mask\n",
        "\n",
        "        attention_probs = F.softmax(attn_scores, dim=-1)\n",
        "\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        # (b, h, t, d_v) = (b, h, t, t) @ (b, h, t, d_v)\n",
        "        # (b, t, h, d_v) <- (b, h, t, d_v)\n",
        "        context = torch.matmul(attention_probs, v).transpose(1, 2).contiguous()\n",
        "\n",
        "        context = context.view(batch_size, seq_len, hidden_size)\n",
        "\n",
        "        return self.w_o(context)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "YDJ2kn59iev0"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadAttention(config)\n",
        "        self.feed_forward = FeedForward(config)\n",
        "        self.ffn_norm = nn.LayerNorm(config.hidden_size, config.layer_norm_eps)\n",
        "        self.attention_norm = nn.LayerNorm(config.hidden_size, config.layer_norm_eps)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, freqs_cis: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        h = x + self.attention(self.attention_norm(x), freqs_cis, attention_mask)\n",
        "        out = h + self.feed_forward(self.ffn_norm(h))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "B_ZpxjB0ie6S"
      },
      "outputs": [],
      "source": [
        "class RoFormer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.token_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
        "        self.layers = nn.ModuleList([EncoderLayer(config) for i in range(config.n_layers)])\n",
        "        self.output = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
        "\n",
        "        self.freqs_cis = precompute_freqs_cis(config.block_size, config.hidden_size // config.n_heads, config.rope_base)\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            x: torch.Tensor,\n",
        "            attention_mask: Optional[torch.Tensor] = None,\n",
        "            input_pos: Optional[torch.Tensor] = None # (0, seq_len-1)\n",
        "    ) -> torch.Tensor:\n",
        "\n",
        "        batch_size, seq_length = x.size()\n",
        "        freqs_cis = self.freqs_cis[input_pos]\n",
        "\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones(((batch_size, seq_length)), device=x.device)\n",
        "        attention_mask = self.get_extended_attention_mask(attention_mask, x.shape)\n",
        "\n",
        "        x = self.token_embeddings(x)\n",
        "\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = layer(x, freqs_cis, attention_mask)\n",
        "        logits = self.output(x)\n",
        "        return logits\n",
        "\n",
        "    def get_extended_attention_mask(self, attention_mask: torch.Tensor, input_shape) -> torch.Tensor:\n",
        "        # attention_mask is 1.0 for positions we want to attend and 0.0 for masked positions\n",
        "        # create attention mask of shape (batch_size, 1, 1, seq_len), where 1 will be 0 and\n",
        "        # 0 will the smallest value for a given dtype (-inf)\n",
        "        extended_attention_mask = attention_mask[:, None, None, :] #\n",
        "        extended_attention_mask = extended_attention_mask.to(dtype=attention_mask.dtype)\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * torch.finfo(attention_mask.dtype).min\n",
        "        return extended_attention_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "CIvKAOcMj85J"
      },
      "outputs": [],
      "source": [
        "roformer = RoFormer(config).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "3ZPkoZeIMZ88"
      },
      "outputs": [],
      "source": [
        "x = torch.randint(0, config.vocab_size, (4, config.block_size), device=device).to(torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9VyUK6UMd9n",
        "outputId": "018acf45-7ce1-451d-bc51-4415fd8fea52"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "RoFormer                                 [4, 1024, 32000]          --\n",
              "├─Embedding: 1-1                         [4, 1024, 768]            24,576,000\n",
              "├─ModuleList: 1-2                        --                        --\n",
              "│    └─EncoderLayer: 2-1                 [4, 1024, 768]            --\n",
              "│    │    └─LayerNorm: 3-1               [4, 1024, 768]            1,536\n",
              "│    │    └─MultiHeadAttention: 3-2      [4, 1024, 768]            2,362,368\n",
              "│    │    └─LayerNorm: 3-3               [4, 1024, 768]            1,536\n",
              "│    │    └─FeedForward: 3-4             [4, 1024, 768]            4,723,968\n",
              "│    └─EncoderLayer: 2-2                 [4, 1024, 768]            --\n",
              "│    │    └─LayerNorm: 3-5               [4, 1024, 768]            1,536\n",
              "│    │    └─MultiHeadAttention: 3-6      [4, 1024, 768]            2,362,368\n",
              "│    │    └─LayerNorm: 3-7               [4, 1024, 768]            1,536\n",
              "│    │    └─FeedForward: 3-8             [4, 1024, 768]            4,723,968\n",
              "│    └─EncoderLayer: 2-3                 [4, 1024, 768]            --\n",
              "│    │    └─LayerNorm: 3-9               [4, 1024, 768]            1,536\n",
              "│    │    └─MultiHeadAttention: 3-10     [4, 1024, 768]            2,362,368\n",
              "│    │    └─LayerNorm: 3-11              [4, 1024, 768]            1,536\n",
              "│    │    └─FeedForward: 3-12            [4, 1024, 768]            4,723,968\n",
              "│    └─EncoderLayer: 2-4                 [4, 1024, 768]            --\n",
              "│    │    └─LayerNorm: 3-13              [4, 1024, 768]            1,536\n",
              "│    │    └─MultiHeadAttention: 3-14     [4, 1024, 768]            2,362,368\n",
              "│    │    └─LayerNorm: 3-15              [4, 1024, 768]            1,536\n",
              "│    │    └─FeedForward: 3-16            [4, 1024, 768]            4,723,968\n",
              "├─Linear: 1-3                            [4, 1024, 32000]          24,576,000\n",
              "==========================================================================================\n",
              "Total params: 77,509,632\n",
              "Trainable params: 77,509,632\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 310.04\n",
              "==========================================================================================\n",
              "Input size (MB): 0.03\n",
              "Forward/backward pass size (MB): 2281.70\n",
              "Params size (MB): 310.04\n",
              "Estimated Total Size (MB): 2591.77\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summary(roformer, input_data=x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "gpKe6m1q38_I"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
