{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc51212b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct Preference Optimization: Your Language Model is Secretly a Reward Model\n",
    "# https://arxiv.org/abs/2305.18290"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "660a0cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6131b230",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathcal{L}_{\\mathrm{DPO}}\\left(\\pi_\\theta ; \\pi_{\\mathrm{ref}}\\right)=-\\mathbb{E}_{\\left(x, y_w, y_l\\right) \\sim \\mathcal{D}}\\left[\\log \\sigma\\left(\\beta \\log \\frac{\\pi_\\theta\\left(y_w \\mid x\\right)}{\\pi_{\\mathrm{ref}}\\left(y_w \\mid x\\right)}-\\beta \\log \\frac{\\pi_\\theta\\left(y_l \\mid x\\right)}{\\pi_{\\mathrm{ref}}\\left(y_l \\mid x\\right)}\\right)\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e60c82",
   "metadata": {},
   "source": [
    "where,\n",
    "- $\\pi_\\theta$ policy model (LLM to optimize)\n",
    "- $ \\pi_{\\mathrm{ref}}$ reference model (original LLM before optimization)\n",
    "- $\\mathbb{E}$ is expected value\n",
    "- $\\sigma$ logistsic sigmoid function\n",
    "- $\\beta$ hyperparameter to control the divergence between the $\\pi_{\\theta}$ and $\\pi_{ref}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7f42f8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3d5b7a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"jondurbin/py-dpo-v0.1\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d3ac069d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "aed21cab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'chosen', 'rejected', 'id'],\n",
       "        num_rows: 8519\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt', 'chosen', 'rejected', 'id'],\n",
       "        num_rows: 947\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c23e0943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \n",
      "What are some efficient algorithms in Python for finding the intersection between two sets of geographic coordinates within a given radius?\n",
      "\n",
      "\n",
      "Chosen: \n",
      "One possible algorithm for finding the intersection between two sets of geographic coordinates within a given radius in Python is as follows:\n",
      "\n",
      "1. Define a function that calculates the great circle distance between two points on the Earth's surface. This can be done using the Haversine formula, which takes into account the Earth's radius and the latitude and longitude of the two points.\n",
      "\n",
      "2. Define a function that takes two sets of geographic coordinates (as lists of latitude and longitude pairs) and a radius as input, and returns the intersection between the two sets within the given radius. This can be done by looping over all pairs of points from the two sets and checking whether the great circle distance between them is less than the given radius. If so, add the pair to a list of intersecting points.\n",
      "\n",
      "3. Use this function to find the intersection between the two sets of coordinates.\n",
      "\n",
      "Here's an implementation of this algorithm in Python:\n",
      "\n",
      "```python\n",
      "import math\n",
      "\n",
      "def haversine(lat1, lon1, lat2, lon2):\n",
      "    # Convert latitude and longitude to radians\n",
      "    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n",
      "\n",
      "    # Haversine formula\n",
      "    dlat = lat2 - lat1\n",
      "    dlon = lon2 - lon1\n",
      "    a = math.sin(dlat/2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon/2)**2\n",
      "    c = 2 * math.asin(math.sqrt(a))\n",
      "    r = 6371 # Radius of the Earth in kilometers\n",
      "    return c * r\n",
      "\n",
      "def find_intersection(coords1, coords2, radius):\n",
      "    intersection = []\n",
      "    for coord1 in coords1:\n",
      "        for coord2 in coords2:\n",
      "            if haversine(coord1[0], coord1[1], coord2[0], coord2[1]) <= radius:\n",
      "                intersection.append((coord1, coord2))\n",
      "    return intersection\n",
      "\n",
      "# Example usage\n",
      "coords1 = [(40.748817, -73.985428), (37.774929, -122.419416), (51.507351, -0.127758)]\n",
      "coords2 = [(48.856614, 2.352222), (34.052234, -118.243685), (55.755826, 37.617299)]\n",
      "radius = 5000 # 5 km\n",
      "intersection = find_intersection(coords1, coords2, radius)\n",
      "print(intersection)\n",
      "```\n",
      "\n",
      "This implementation uses the Haversine formula to calculate the great circle distance between pairs of points on the Earth's surface, and checks whether the distance is less than or equal to the given radius. It returns a list of pairs of points that are within the given radius of each other.\n",
      "\n",
      "\n",
      "Rejected: \n",
      "To find the intersection between two sets of geographic coordinates within a given radius, we can use the Haversine formula. The Haversine formula is a great tool for calculating the distance between two points on a sphere, which is what we need to do in this case.\n",
      "\n",
      "Here is an efficient algorithm in Python for finding the intersection between two sets of geographic coordinates within a given radius:\n",
      "\n",
      "```python\n",
      "import math\n",
      "\n",
      "def haversine(lat1, lon1, lat2, lon2):\n",
      "    # Convert latitude and longitude to radians\n",
      "    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n",
      "\n",
      "    # Haversine formula\n",
      "    dlon = lon2 - lon1\n",
      "    dlat = lat2 - lat1\n",
      "    a = math.sin(dlat/2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon/2)**2\n",
      "    c = 2 * math.asin(math.sqrt(a))\n",
      "    r = 6371 # Earth's radius in kilometers\n",
      "    return c * r\n",
      "\n",
      "def find_intersection(coordinates1, coordinates2, radius):\n",
      "    # Convert coordinates to latitude and longitude\n",
      "    coordinates1 = [(lat, lon) for lat, lon in coordinates1]\n",
      "    coordinates2 = [(lat, lon) for lat, lon in coordinates2]\n",
      "\n",
      "    # Calculate the distance between each pair of coordinates\n",
      "    distances = [haversine(lat1, lon1, lat2, lon2) for lat1, lon1, lat2, lon2 in zip(coordinates1, coordinates2, coordinates2, coordinates1)]\n",
      "\n",
      "    # Filter out coordinates that are within the given radius\n",
      "    filtered_coordinates = [coordinate for coordinate, distance in zip(coordinates1, distances) if distance <= radius]\n",
      "\n",
      "    return filtered_coordinates\n",
      "```\n",
      "\n",
      "To use this algorithm, you can call the `find_intersection` function with the two sets of coordinates and the desired radius. For example:\n",
      "\n",
      "```python\n",
      "coordinates1 = [(37.7749, -122.4194), (37.7749, -122.4194), (37.7749, -122.4194)]\n",
      "coordinates2 = [(37.7749, -122.4194), (37.7749, -122.4194), (37.7749, -122.4194)]\n",
      "radius = 1000 # 1 kilometer\n",
      "\n",
      "intersection = find_intersection(coordinates1, coordinates2, radius)\n",
      "print(intersection)\n",
      "```\n",
      "\n",
      "This will output the coordinates that are within 1 kilometer of each other.\n"
     ]
    }
   ],
   "source": [
    "print(\"Prompt: \")\n",
    "print(dataset[\"train\"][0][\"prompt\"])\n",
    "print(\"\\n\\nChosen: \")\n",
    "print(dataset[\"train\"][0][\"chosen\"])\n",
    "print(\"\\n\\nRejected: \")\n",
    "print(dataset[\"train\"][0][\"rejected\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3ab6f1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0c1e9e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['prompt']}\"\n",
    "    )\n",
    "    return instruction_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b0663b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn(entry: dict):\n",
    "\n",
    "    prompt = format_input(entry)\n",
    "    chosen_response = entry[\"chosen\"]\n",
    "    rejected_response = entry[\"rejected\"]\n",
    "\n",
    "    chosen_full_text = f\"{prompt}\\n\\n### Response:\\n{chosen_response}\"\n",
    "    rejected_full_text = f\"{prompt}\\n\\n### Response:\\n{rejected_response}\"\n",
    "\n",
    "    prompt_tokens = tokenizer(chosen_full_text, truncation=True, padding=False, max_length=1024, return_attention_mask=False)[\"input_ids\"]\n",
    "    chosen_full_tokens = tokenizer(chosen_full_text, truncation=True, padding=False, max_length=1024, return_attention_mask=False)[\"input_ids\"]\n",
    "    rejected_full_tokens = tokenizer(rejected_full_text, truncation=True, padding=False, max_length=1024, return_attention_mask=False)[\"input_ids\"]\n",
    "\n",
    "    return { \"prompt\": prompt_tokens, \"chosen\": chosen_full_tokens, \"rejected\": rejected_full_tokens }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "806742bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/947 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 947/947 [00:04<00:00, 221.94 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(tokenize_fn, remove_columns=[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "279aaa34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \n",
      "[21106, 318, 281, 12064, 326, 8477, 257, 4876, 13, 19430, 257, 2882, 326, 20431, 32543, 262, 2581, 13, 198, 198, 21017, 46486, 25, 198, 2061, 389, 617, 6942, 16113, 287, 11361, 329, 4917, 262, 16246, 1022, 734, 5621, 286, 22987, 22715, 1626, 257, 1813, 16874, 30, 198, 198, 21017, 18261, 25, 198, 3198, 1744, 11862, 329, 4917, 262, 16246, 1022, 734, 5621, 286, 22987, 22715, 1626, 257, 1813, 16874, 287, 11361, 318, 355, 5679, 25, 198, 198, 16, 13, 2896, 500, 257, 2163, 326, 43707, 262, 1049, 9197, 5253, 1022, 734, 2173, 319, 262, 3668, 338, 4417, 13, 770, 460, 307, 1760, 1262, 262, 9398, 690, 500, 10451, 11, 543, 2753, 656, 1848, 262, 3668, 338, 16874, 290, 262, 32477, 290, 890, 3984, 286, 262, 734, 2173, 13, 198, 198, 17, 13, 2896, 500, 257, 2163, 326, 2753, 734, 5621, 286, 22987, 22715, 357, 292, 8341, 286, 32477, 290, 890, 3984, 14729, 8, 290, 257, 16874, 355, 5128, 11, 290, 5860, 262, 16246, 1022, 262, 734, 5621, 1626, 262, 1813, 16874, 13, 770, 460, 307, 1760, 416, 9052, 278, 625, 477, 14729, 286, 2173, 422, 262, 734, 5621, 290, 10627, 1771, 262, 1049, 9197, 5253, 1022, 606, 318, 1342, 621, 262, 1813, 16874, 13, 1002, 523, 11, 751, 262, 5166, 284, 257, 1351, 286, 36177, 278, 2173, 13, 198, 198, 18, 13, 5765, 428, 2163, 284, 1064, 262, 16246, 1022, 262, 734, 5621, 286, 22715, 13, 198, 198, 4342, 338, 281, 7822, 286, 428, 11862, 287, 11361, 25, 198, 198, 15506, 63, 29412, 198, 11748, 10688, 198, 198, 4299, 387, 690, 500, 7, 15460, 16, 11, 300, 261, 16, 11, 3042, 17, 11, 300, 261, 17, 2599, 198, 220, 220, 220, 1303, 38240, 32477, 290, 890, 3984, 284, 2511, 1547, 198, 220, 220, 220, 3042, 16, 11, 300, 261, 16, 11, 3042, 17, 11, 300, 261, 17, 796, 3975, 7, 11018, 13, 6335, 1547, 11, 685, 15460, 16, 11, 300, 261, 16, 11, 3042, 17, 11, 300, 261, 17, 12962, 628, 220, 220, 220, 1303, 9398, 690, 500, 10451, 198, 220, 220, 220, 288, 15460, 796, 3042, 17, 532, 3042, 16, 198, 220, 220, 220, 288, 14995, 796, 300, 261, 17, 532, 300, 261, 16, 198, 220, 220, 220, 257, 796, 10688, 13, 31369, 7, 67, 15460, 14, 17, 8, 1174, 17, 1343, 10688, 13, 6966, 7, 15460, 16, 8, 1635, 10688, 13, 6966, 7, 15460, 17, 8, 1635, 10688, 13, 31369, 7, 67, 14995, 14, 17, 8, 1174, 17, 198, 220, 220, 220, 269, 796, 362, 1635, 10688, 13, 47337, 7, 11018, 13, 31166, 17034, 7, 64, 4008, 198, 220, 220, 220, 374, 796, 718, 38056, 1303, 48838, 286, 262, 3668, 287, 18212, 198, 220, 220, 220, 1441, 269, 1635, 374, 198, 198, 4299, 1064, 62, 3849, 5458, 7, 1073, 3669, 16, 11, 763, 3669, 17, 11, 16874, 2599, 198, 220, 220, 220, 16246, 796, 17635, 198, 220, 220, 220, 329, 6349, 16, 287, 763, 3669, 16, 25, 198, 220, 220, 220, 220, 220, 220, 220, 329, 6349, 17, 287, 763, 3669, 17, 25, 198, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 611, 387, 690, 500, 7, 37652, 16, 58, 15, 4357, 6349, 16, 58, 16, 4357, 6349, 17, 58, 15, 4357, 6349, 17, 58, 16, 12962, 19841, 16874, 25, 198, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 16246, 13, 33295, 19510, 37652, 16, 11, 6349, 17, 4008, 198, 220, 220, 220, 1441, 16246, 198, 198, 2, 17934, 8748, 198, 1073, 3669, 16, 796, 47527, 1821, 13, 22, 33646, 1558, 11, 532, 4790, 13, 4089, 4051, 2078, 828, 357, 2718, 13, 3324, 2920, 1959, 11, 532, 18376, 13, 19, 22913, 1433, 828, 357, 4349, 13, 35378, 35273, 11, 532, 15, 13, 16799, 38569, 15437, 198, 1073, 3669, 17, 796, 47527, 2780, 13, 5332, 2791, 1415, 11, 362, 13, 2327, 1828, 1828, 828, 357, 2682, 13, 2713, 1828, 2682, 11, 532, 16817, 13, 1731, 2623, 5332, 828, 357, 2816, 13, 2425, 3365, 2075, 11, 5214, 13, 47941, 22579, 15437, 198, 42172, 796, 23336, 1303, 642, 10571, 198, 3849, 5458, 796, 1064, 62, 3849, 5458, 7, 1073, 3669, 16, 11, 763, 3669, 17, 11, 16874, 8, 198, 4798, 7, 3849, 5458, 8, 198, 15506, 63, 198, 198, 1212, 7822, 3544, 262, 9398, 690, 500, 10451, 284, 15284, 262, 1049, 9197, 5253, 1022, 14729, 286, 2173, 319, 262, 3668, 338, 4417, 11, 290, 8794, 1771, 262, 5253, 318, 1342, 621, 393, 4961, 284, 262, 1813, 16874, 13, 632, 5860, 257, 1351, 286, 14729, 286, 2173, 326, 389, 1626, 262, 1813, 16874, 286, 1123, 584, 13]\n",
      "\n",
      "\n",
      "Chosen: \n",
      "[21106, 318, 281, 12064, 326, 8477, 257, 4876, 13, 19430, 257, 2882, 326, 20431, 32543, 262, 2581, 13, 198, 198, 21017, 46486, 25, 198, 2061, 389, 617, 6942, 16113, 287, 11361, 329, 4917, 262, 16246, 1022, 734, 5621, 286, 22987, 22715, 1626, 257, 1813, 16874, 30, 198, 198, 21017, 18261, 25, 198, 3198, 1744, 11862, 329, 4917, 262, 16246, 1022, 734, 5621, 286, 22987, 22715, 1626, 257, 1813, 16874, 287, 11361, 318, 355, 5679, 25, 198, 198, 16, 13, 2896, 500, 257, 2163, 326, 43707, 262, 1049, 9197, 5253, 1022, 734, 2173, 319, 262, 3668, 338, 4417, 13, 770, 460, 307, 1760, 1262, 262, 9398, 690, 500, 10451, 11, 543, 2753, 656, 1848, 262, 3668, 338, 16874, 290, 262, 32477, 290, 890, 3984, 286, 262, 734, 2173, 13, 198, 198, 17, 13, 2896, 500, 257, 2163, 326, 2753, 734, 5621, 286, 22987, 22715, 357, 292, 8341, 286, 32477, 290, 890, 3984, 14729, 8, 290, 257, 16874, 355, 5128, 11, 290, 5860, 262, 16246, 1022, 262, 734, 5621, 1626, 262, 1813, 16874, 13, 770, 460, 307, 1760, 416, 9052, 278, 625, 477, 14729, 286, 2173, 422, 262, 734, 5621, 290, 10627, 1771, 262, 1049, 9197, 5253, 1022, 606, 318, 1342, 621, 262, 1813, 16874, 13, 1002, 523, 11, 751, 262, 5166, 284, 257, 1351, 286, 36177, 278, 2173, 13, 198, 198, 18, 13, 5765, 428, 2163, 284, 1064, 262, 16246, 1022, 262, 734, 5621, 286, 22715, 13, 198, 198, 4342, 338, 281, 7822, 286, 428, 11862, 287, 11361, 25, 198, 198, 15506, 63, 29412, 198, 11748, 10688, 198, 198, 4299, 387, 690, 500, 7, 15460, 16, 11, 300, 261, 16, 11, 3042, 17, 11, 300, 261, 17, 2599, 198, 220, 220, 220, 1303, 38240, 32477, 290, 890, 3984, 284, 2511, 1547, 198, 220, 220, 220, 3042, 16, 11, 300, 261, 16, 11, 3042, 17, 11, 300, 261, 17, 796, 3975, 7, 11018, 13, 6335, 1547, 11, 685, 15460, 16, 11, 300, 261, 16, 11, 3042, 17, 11, 300, 261, 17, 12962, 628, 220, 220, 220, 1303, 9398, 690, 500, 10451, 198, 220, 220, 220, 288, 15460, 796, 3042, 17, 532, 3042, 16, 198, 220, 220, 220, 288, 14995, 796, 300, 261, 17, 532, 300, 261, 16, 198, 220, 220, 220, 257, 796, 10688, 13, 31369, 7, 67, 15460, 14, 17, 8, 1174, 17, 1343, 10688, 13, 6966, 7, 15460, 16, 8, 1635, 10688, 13, 6966, 7, 15460, 17, 8, 1635, 10688, 13, 31369, 7, 67, 14995, 14, 17, 8, 1174, 17, 198, 220, 220, 220, 269, 796, 362, 1635, 10688, 13, 47337, 7, 11018, 13, 31166, 17034, 7, 64, 4008, 198, 220, 220, 220, 374, 796, 718, 38056, 1303, 48838, 286, 262, 3668, 287, 18212, 198, 220, 220, 220, 1441, 269, 1635, 374, 198, 198, 4299, 1064, 62, 3849, 5458, 7, 1073, 3669, 16, 11, 763, 3669, 17, 11, 16874, 2599, 198, 220, 220, 220, 16246, 796, 17635, 198, 220, 220, 220, 329, 6349, 16, 287, 763, 3669, 16, 25, 198, 220, 220, 220, 220, 220, 220, 220, 329, 6349, 17, 287, 763, 3669, 17, 25, 198, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 611, 387, 690, 500, 7, 37652, 16, 58, 15, 4357, 6349, 16, 58, 16, 4357, 6349, 17, 58, 15, 4357, 6349, 17, 58, 16, 12962, 19841, 16874, 25, 198, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 16246, 13, 33295, 19510, 37652, 16, 11, 6349, 17, 4008, 198, 220, 220, 220, 1441, 16246, 198, 198, 2, 17934, 8748, 198, 1073, 3669, 16, 796, 47527, 1821, 13, 22, 33646, 1558, 11, 532, 4790, 13, 4089, 4051, 2078, 828, 357, 2718, 13, 3324, 2920, 1959, 11, 532, 18376, 13, 19, 22913, 1433, 828, 357, 4349, 13, 35378, 35273, 11, 532, 15, 13, 16799, 38569, 15437, 198, 1073, 3669, 17, 796, 47527, 2780, 13, 5332, 2791, 1415, 11, 362, 13, 2327, 1828, 1828, 828, 357, 2682, 13, 2713, 1828, 2682, 11, 532, 16817, 13, 1731, 2623, 5332, 828, 357, 2816, 13, 2425, 3365, 2075, 11, 5214, 13, 47941, 22579, 15437, 198, 42172, 796, 23336, 1303, 642, 10571, 198, 3849, 5458, 796, 1064, 62, 3849, 5458, 7, 1073, 3669, 16, 11, 763, 3669, 17, 11, 16874, 8, 198, 4798, 7, 3849, 5458, 8, 198, 15506, 63, 198, 198, 1212, 7822, 3544, 262, 9398, 690, 500, 10451, 284, 15284, 262, 1049, 9197, 5253, 1022, 14729, 286, 2173, 319, 262, 3668, 338, 4417, 11, 290, 8794, 1771, 262, 5253, 318, 1342, 621, 393, 4961, 284, 262, 1813, 16874, 13, 632, 5860, 257, 1351, 286, 14729, 286, 2173, 326, 389, 1626, 262, 1813, 16874, 286, 1123, 584, 13]\n",
      "\n",
      "\n",
      "Rejected: \n",
      "[21106, 318, 281, 12064, 326, 8477, 257, 4876, 13, 19430, 257, 2882, 326, 20431, 32543, 262, 2581, 13, 198, 198, 21017, 46486, 25, 198, 2061, 389, 617, 6942, 16113, 287, 11361, 329, 4917, 262, 16246, 1022, 734, 5621, 286, 22987, 22715, 1626, 257, 1813, 16874, 30, 198, 198, 21017, 18261, 25, 198, 2514, 1064, 262, 16246, 1022, 734, 5621, 286, 22987, 22715, 1626, 257, 1813, 16874, 11, 356, 460, 779, 262, 9398, 690, 500, 10451, 13, 383, 9398, 690, 500, 10451, 318, 257, 1049, 2891, 329, 26019, 262, 5253, 1022, 734, 2173, 319, 257, 16558, 11, 543, 318, 644, 356, 761, 284, 466, 287, 428, 1339, 13, 198, 198, 4342, 318, 281, 6942, 11862, 287, 11361, 329, 4917, 262, 16246, 1022, 734, 5621, 286, 22987, 22715, 1626, 257, 1813, 16874, 25, 198, 198, 15506, 63, 29412, 198, 11748, 10688, 198, 198, 4299, 387, 690, 500, 7, 15460, 16, 11, 300, 261, 16, 11, 3042, 17, 11, 300, 261, 17, 2599, 198, 220, 220, 220, 1303, 38240, 32477, 290, 890, 3984, 284, 2511, 1547, 198, 220, 220, 220, 3042, 16, 11, 300, 261, 16, 11, 3042, 17, 11, 300, 261, 17, 796, 3975, 7, 11018, 13, 6335, 1547, 11, 685, 15460, 16, 11, 300, 261, 16, 11, 3042, 17, 11, 300, 261, 17, 12962, 628, 220, 220, 220, 1303, 9398, 690, 500, 10451, 198, 220, 220, 220, 288, 14995, 796, 300, 261, 17, 532, 300, 261, 16, 198, 220, 220, 220, 288, 15460, 796, 3042, 17, 532, 3042, 16, 198, 220, 220, 220, 257, 796, 10688, 13, 31369, 7, 67, 15460, 14, 17, 8, 1174, 17, 1343, 10688, 13, 6966, 7, 15460, 16, 8, 1635, 10688, 13, 6966, 7, 15460, 17, 8, 1635, 10688, 13, 31369, 7, 67, 14995, 14, 17, 8, 1174, 17, 198, 220, 220, 220, 269, 796, 362, 1635, 10688, 13, 47337, 7, 11018, 13, 31166, 17034, 7, 64, 4008, 198, 220, 220, 220, 374, 796, 718, 38056, 1303, 3668, 338, 16874, 287, 18212, 198, 220, 220, 220, 1441, 269, 1635, 374, 198, 198, 4299, 1064, 62, 3849, 5458, 7, 37652, 17540, 16, 11, 22715, 17, 11, 16874, 2599, 198, 220, 220, 220, 1303, 38240, 22715, 284, 32477, 290, 890, 3984, 198, 220, 220, 220, 22715, 16, 796, 47527, 15460, 11, 300, 261, 8, 329, 3042, 11, 300, 261, 287, 22715, 16, 60, 198, 220, 220, 220, 22715, 17, 796, 47527, 15460, 11, 300, 261, 8, 329, 3042, 11, 300, 261, 287, 22715, 17, 60, 628, 220, 220, 220, 1303, 27131, 378, 262, 5253, 1022, 1123, 5166, 286, 22715, 198, 220, 220, 220, 18868, 796, 685, 3099, 690, 500, 7, 15460, 16, 11, 300, 261, 16, 11, 3042, 17, 11, 300, 261, 17, 8, 329, 3042, 16, 11, 300, 261, 16, 11, 3042, 17, 11, 300, 261, 17, 287, 19974, 7, 37652, 17540, 16, 11, 22715, 17, 11, 22715, 17, 11, 22715, 16, 15437, 628, 220, 220, 220, 1303, 25853, 503, 22715, 326, 389, 1626, 262, 1813, 16874, 198, 220, 220, 220, 29083, 62, 37652, 17540, 796, 685, 37652, 4559, 329, 20435, 11, 5253, 287, 19974, 7, 37652, 17540, 16, 11, 18868, 8, 611, 5253, 19841, 16874, 60, 628, 220, 220, 220, 1441, 29083, 62, 37652, 17540, 198, 15506, 63, 198, 198, 2514, 779, 428, 11862, 11, 345, 460, 869, 262, 4600, 19796, 62, 3849, 5458, 63, 2163, 351, 262, 734, 5621, 286, 22715, 290, 262, 10348, 16874, 13, 1114, 1672, 25, 198, 198, 15506, 63, 29412, 198, 37652, 17540, 16, 796, 47527, 2718, 13, 3324, 2920, 11, 532, 18376, 13, 19, 22913, 828, 357, 2718, 13, 3324, 2920, 11, 532, 18376, 13, 19, 22913, 828, 357, 2718, 13, 3324, 2920, 11, 532, 18376, 13, 19, 22913, 15437, 198, 37652, 17540, 17, 796, 47527, 2718, 13, 3324, 2920, 11, 532, 18376, 13, 19, 22913, 828, 357, 2718, 13, 3324, 2920, 11, 532, 18376, 13, 19, 22913, 828, 357, 2718, 13, 3324, 2920, 11, 532, 18376, 13, 19, 22913, 15437, 198, 42172, 796, 8576, 1303, 352, 11866, 263, 198, 198, 3849, 5458, 796, 1064, 62, 3849, 5458, 7, 37652, 17540, 16, 11, 22715, 17, 11, 16874, 8, 198, 4798, 7, 3849, 5458, 8, 198, 15506, 63, 198, 198, 1212, 481, 5072, 262, 22715, 326, 389, 1626, 352, 11866, 263, 286, 1123, 584, 13]\n"
     ]
    }
   ],
   "source": [
    "print(\"Prompt: \")\n",
    "print(tokenized_dataset[\"train\"][0][\"prompt\"])\n",
    "print(\"\\n\\nChosen: \")\n",
    "print(tokenized_dataset[\"train\"][0][\"chosen\"])\n",
    "print(\"\\n\\nRejected: \")\n",
    "print(tokenized_dataset[\"train\"][0][\"rejected\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "29756971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(\n",
    "    batch,\n",
    "    pad_token_id = 50256,\n",
    "    context_length = None,\n",
    "    mask_prompt_tokens = True,\n",
    "    device = \"cpu\",\n",
    "):\n",
    "\n",
    "    batch_data = {\n",
    "        \"prompt\": [],\n",
    "        \"chosen\": [],\n",
    "        \"rejected\": [],\n",
    "        \"rejected_mask\": [],\n",
    "        \"chosen_mask\": []\n",
    "\n",
    "    }\n",
    "\n",
    "    # dynamic padding\n",
    "    max_length_common = 0\n",
    "    for key in [\"chosen\", \"rejected\"]:\n",
    "        current_max = max(len(item[key])+1 for item in batch)\n",
    "        max_length_common = max(max_length_common, current_max)\n",
    "\n",
    "    for item in batch:\n",
    "        prompt = torch.tensor(item[\"prompt\"])\n",
    "        batch_data[\"prompt\"].append(prompt)\n",
    "\n",
    "        for key in [\"chosen\", \"rejected\"]:\n",
    "\n",
    "            # adjust padding according to the common maximum length\n",
    "            sequence = item[key]\n",
    "            padded = sequence + [pad_token_id] * (max_length_common - len(sequence))\n",
    "            mask = torch.ones(len(padded)).bool()\n",
    "\n",
    "            # set mask for padding tokens to false after sequence ends\n",
    "            mask[len(sequence):] = False\n",
    "\n",
    "            # set mask for all prompt tokens to false\n",
    "            # +2 sets the 2 newline (\"\\n\") tokens before \"### Response\" to false\n",
    "            if mask_prompt_tokens:\n",
    "                mask[:prompt.shape[0]+2] = False\n",
    "\n",
    "            batch_data[key].append(torch.tensor(padded))\n",
    "            batch_data[f\"{key}_mask\"].append(mask)\n",
    "\n",
    "    for key in [\"chosen\", \"rejected\", \"chosen_mask\", \"rejected_mask\"]:\n",
    "        # stack all sequences into a tensor for the given key\n",
    "        tensor_stack = torch.stack(batch_data[key])\n",
    "\n",
    "        # truncate to maximum sequence length\n",
    "        if context_length is not None:\n",
    "            tensor_stack = tensor_stack[:, :context_length]\n",
    "\n",
    "        batch_data[key] = tensor_stack.to(device)\n",
    "\n",
    "    return batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "19baaf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_collate_fn = partial(\n",
    "    collate_fn,\n",
    "    device=device,\n",
    "    mask_prompt_tokens=True,\n",
    "    context_length=1024 # model context length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b7d4c24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    tokenized_dataset[\"train\"],\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    collate_fn=custom_collate_fn,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    tokenized_dataset[\"test\"],\n",
    "    batch_size=8,\n",
    "    collate_fn=custom_collate_fn,\n",
    "    num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e9091629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/osvathm/ai-notebooks/venv/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 309, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/osvathm/ai-notebooks/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n  File \"/tmp/ipykernel_1330621/285878571.py\", line 54, in collate_fn\n    batch_data[key] = tensor_stack.to(device)\n  File \"/home/osvathm/ai-notebooks/venv/lib/python3.8/site-packages/torch/cuda/__init__.py\", line 300, in _lazy_init\n    raise RuntimeError(\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain loader:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m      4\u001b[0m         batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchosen\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape,\n\u001b[1;32m      5\u001b[0m         batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrejected\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape,\n\u001b[1;32m      6\u001b[0m     )\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m idx\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m10\u001b[39m:\n",
      "File \u001b[0;32m~/ai-notebooks/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/ai-notebooks/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1344\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1343\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ai-notebooks/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1370\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1368\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1370\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/ai-notebooks/venv/lib/python3.8/site-packages/torch/_utils.py:706\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 706\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/osvathm/ai-notebooks/venv/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 309, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/osvathm/ai-notebooks/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n  File \"/tmp/ipykernel_1330621/285878571.py\", line 54, in collate_fn\n    batch_data[key] = tensor_stack.to(device)\n  File \"/home/osvathm/ai-notebooks/venv/lib/python3.8/site-packages/torch/cuda/__init__.py\", line 300, in _lazy_init\n    raise RuntimeError(\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for idx, batch in enumerate(train_loader):\n",
    "    print(\n",
    "        batch[\"chosen\"].shape,\n",
    "        batch[\"rejected\"].shape,\n",
    "    )\n",
    "    if idx>10:\n",
    "        break\n",
    "\n",
    "print(\"\\nTest loader:\")\n",
    "for idx, batch in enumerate(test_loader):\n",
    "    print(\n",
    "        batch[\"chosen\"].shape,\n",
    "        batch[\"rejected\"].shape,\n",
    "    )\n",
    "    if idx>10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219da855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dae32df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logprobs(\n",
    "    logits, # (batch_size, num_tokens, vocab_size)\n",
    "    labels, # (batch_size, num_tokens)\n",
    "    selection_mask=None # (batch_size, num_tokens)\n",
    "):\n",
    "\n",
    "    # labels are the inputs shifted by one\n",
    "    #   i.e. the model predicts the next token in the sequence,\n",
    "    #   so the labels are the input tokens shifted to the left\n",
    "    #   and the first token is ignored (because there's no previous token to predict)\n",
    "    labels = labels[:, 1:].clone()\n",
    "\n",
    "    # adjust logits to match the labels num_tokens\n",
    "    #   i.e. truncate the logits by removing the last token's logits\n",
    "    #   because there's no corresponding target (label) to predict for the last token.\n",
    "    logits = logits[:, :-1, :]\n",
    "\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "    # select log probability the model assigned to the correct token at each position\n",
    "    selected_log_probs = torch.gather(\n",
    "        input=log_probs,\n",
    "        dim=-1,\n",
    "        index=labels.unsqueeze(-1)\n",
    "    ).squeeze(-1)\n",
    "\n",
    "    print(\"Log probs:\", selected_log_probs)\n",
    "    print(\"Log probs shape:\", selected_log_probs.shape)\n",
    "\n",
    "    if selection_mask is not None:\n",
    "        mask = selection_mask[:, 1:].clone()\n",
    "\n",
    "        print(\"Mask:\", mask)\n",
    "        print(\"Mask shape:\", mask.shape)\n",
    "\n",
    "        # apply the mask to filter out padding tokens\n",
    "        #    i.e. set the log probabilities of padding tokens to 0\n",
    "        selected_log_probs = selected_log_probs * mask\n",
    "\n",
    "        print(\"selected Log probs:\", selected_log_probs)\n",
    "        print(\"selected Log probs shape:\", selected_log_probs.shape)\n",
    "\n",
    "        # Calculate the average log probability excluding padding tokens\n",
    "        # This averages over the tokens, so the shape is (batch_size, num_tokens)\n",
    "        avg_log_prob = selected_log_probs.sum(-1) / mask.sum(-1)\n",
    "\n",
    "        return avg_log_prob\n",
    "\n",
    "    else:\n",
    "        return selected_log_probs.mean(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "522f66af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log probs: tensor([[-2.3272, -1.9925, -2.3383],\n",
      "        [-1.0608, -1.9837, -2.0889]])\n",
      "Log probs shape: torch.Size([2, 3])\n",
      "Mask: tensor([[1, 1, 1],\n",
      "        [1, 1, 0]])\n",
      "Mask shape: torch.Size([2, 3])\n",
      "selected Log probs: tensor([[-2.3272, -1.9925, -2.3383],\n",
      "        [-1.0608, -1.9837, -0.0000]])\n",
      "selected Log probs shape: torch.Size([2, 3])\n",
      "Average Log Probabilities: tensor([-2.2193, -1.5223])\n"
     ]
    }
   ],
   "source": [
    "# (batch_size, num_tokens, vocab_size)\n",
    "logits = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "            [2.0, 0.1, 0.3, -0.4, 1.2],  # logit for token 1 in batch 1\n",
    "            [1.2, -0.5, 0.9, 0.2, 0.3],  # logit for token 2 in batch 1\n",
    "            [-0.3, 1.4, 0.2, 0.5, -0.2],  # logit for token 3 in batch 1\n",
    "            [0.6, 0.1, -0.3, 0.4, 0.7]    # logit for token 4 in batch 1\n",
    "        ],\n",
    "        [\n",
    "            [-0.2, 0.7, 1.1, 0.3, 0.4],  # logit for token 1 in batch 2\n",
    "            [1.5, -0.8, 0.6, 0.2, -0.1],  # logit for token 2 in batch 2\n",
    "            [0.1, 0.9, 0.4, -0.6, 1.2],  # logit for token 3 in batch 2\n",
    "            [-0.3, 0.2, 0.4, 0.6, -0.2] # logit for token 4 in batch 2\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# (batch_size, num_tokens)\n",
    "labels = torch.tensor(\n",
    "    [\n",
    "        [1, 2, 3, 4],  # True labels for sequence 1\n",
    "        [1, 2, 3, 0]   # True labels for sequence 2 (0 is padding)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# (batch_size, num_tokens)\n",
    "selection_mask = torch.tensor(\n",
    "    [\n",
    "        [1, 1, 1, 1],   # No padding tokens in sequence 1\n",
    "        [1, 1, 1, 0]    # Padding token at the last position in sequence 2\n",
    "    ]\n",
    ")\n",
    "\n",
    "avg_log_probs = compute_logprobs(logits, labels, selection_mask)\n",
    "\n",
    "print(\"Average Log Probabilities:\", avg_log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec121e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 3])\n",
      "torch.Size([1, 2, 1])\n",
      "tensor(2)\n",
      "tensor([[[-1.0000],\n",
      "         [-2.5000]]])\n",
      "torch.Size([1, 2, 1])\n",
      "tensor([[-1.0000, -2.5000]])\n",
      "torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "log_probs = torch.tensor([\n",
    "    [  # batch 0\n",
    "        [-1.0, -2.0, -3.0],  # timestep 0\n",
    "        [-0.1, -0.2, -2.5]   # timestep 1\n",
    "    ]\n",
    "])  # shape: (1, 2, 3)\n",
    "\n",
    "labels = torch.tensor([\n",
    "    [0, 2]\n",
    "])  # shape: (1, 2)\n",
    "\n",
    "\n",
    "print(log_probs.shape)\n",
    "\n",
    "labels = labels.unsqueeze(-1)  # shape: (1, 2, 1)\n",
    "\n",
    "# print(labels)\n",
    "print(labels.shape)\n",
    "print(labels[0,1,0])\n",
    "\n",
    "selected_log_probs = torch.gather(log_probs, dim=-1, index=labels)  # shape: (1, 2, 1)\n",
    "\n",
    "print(selected_log_probs)\n",
    "print(selected_log_probs.shape)\n",
    "\n",
    "selected_log_probs = selected_log_probs.squeeze(-1)  # shape: (1, 2)\n",
    "\n",
    "print(selected_log_probs)\n",
    "print(selected_log_probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c084d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dpo_loss(\n",
    "      model_chosen_logprobs,\n",
    "      model_rejected_logprobs,\n",
    "      reference_chosen_logprobs,\n",
    "      reference_rejected_logprobs,\n",
    "      beta=0.1,\n",
    "    ):\n",
    "\n",
    "    model_logratios = model_chosen_logprobs - model_rejected_logprobs\n",
    "    reference_logratios = reference_chosen_logprobs - reference_rejected_logprobs\n",
    "    logits = model_logratios - reference_logratios\n",
    "\n",
    "    losses = -F.logsigmoid(beta * logits)\n",
    "\n",
    "    # Optional values to track progress during training\n",
    "    chosen_rewards = (model_chosen_logprobs - reference_chosen_logprobs).detach()\n",
    "    rejected_rewards = (model_rejected_logprobs - reference_rejected_logprobs).detach()\n",
    "\n",
    "    # .mean() to average over the samples in the batch\n",
    "    return losses.mean(), chosen_rewards.mean(), rejected_rewards.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4bcf8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
