{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cda13f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61030067",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 2\n",
    "seq_len = 16\n",
    "dim = 512\n",
    "n_heads = 8\n",
    "head_dim = dim // n_heads\n",
    "inputs = torch.randn((bs, seq_len, dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab512eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_freqs_cis(\n",
    "        seq_len: int,\n",
    "        head_dim: int,\n",
    "        base: int = 10_000\n",
    ") -> torch.Tensor:\n",
    "    freqs = 1.0 / (base ** (torch.arange(0, head_dim, 2)[: (head_dim // 2)].float() / head_dim))    # (d // 2)\n",
    "    t = torch.arange(seq_len, device=freqs.device)                                                 # [0, 1, 2, ..., seq_len-1], (seql_len)\n",
    "    freqs = torch.outer(t, freqs)                                                                  # (seq_len, d // 2)\n",
    "    # x' = x * cos(theta) + x * sin(theta) * i, ahol i az imaginárius szám\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)                                         # (seq_len, d // 2)\n",
    "    cache = torch.stack([freqs_cis.real, freqs_cis.imag], dim=-1)                                  # (seq_len, d // 2, 2)\n",
    "    return cache.to(dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c0a9fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def apply_rotary_emb(\n",
    "    x: torch.Tensor,                                                                          # (bs, block_size, n_heads, head_dim)\n",
    "    freqs_cis: torch.Tensor                                                                   # (block_size, head_dim // 2, 2)\n",
    ") -> torch.Tensor:\n",
    "    xshaped = x.float().reshape(*x.shape[:-1], -1, 2)                                    # (bs, block_size, n_heads, head_dim // 2, 2)\n",
    "    freqs_cis = freqs_cis.view(1, xshaped.size(1), 1, xshaped.size(3), 2)               # (1, block_size, 1, head_dim // 2, 2)\n",
    "    x_out = torch.stack(\n",
    "        [\n",
    "            # első komponens rotáció: x1 * cos(theta) - x2 * sin(theta)\n",
    "            xshaped[..., 0] * freqs_cis[..., 0] - xshaped[..., 1] * freqs_cis[..., 1],\n",
    "            # második komponens rotáció: x2 * cos(theta) + x1 * sin(theta)\n",
    "            xshaped[..., 1] * freqs_cis[..., 0] + xshaped[..., 0] * freqs_cis[..., 1],\n",
    "        ],\n",
    "        -1,\n",
    "    )\n",
    "    x_out = x_out.flatten(3)\n",
    "    return x_out.type_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "979eb8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(self, dim: int, n_heads: int, head_dim: int, n_kv_heads: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = head_dim                        # dim // n_heads\n",
    "        self.n_kv_heads = n_kv_heads                    # n groups\n",
    "\n",
    "        self.repeats = self.n_heads // self.n_kv_heads  # group size\n",
    "\n",
    "        self.wq = nn.Linear(dim, n_heads * head_dim, bias=False)\n",
    "        self.wk = nn.Linear(dim, n_kv_heads * head_dim, bias=False)\n",
    "        self.wv = nn.Linear(dim, n_kv_heads * head_dim, bias=False)\n",
    "        self.wo = nn.Linear(n_heads * head_dim, dim, bias=False)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, freqs_cis: torch.Tensor, cache = None, mask = None) -> torch.Tensor:\n",
    "\n",
    "        assert mask is None or cache is None\n",
    "        bs, seq_len, _ = x.shape\n",
    "\n",
    "        q, k, v = self.wq(x), self.wk(x), self.wv(x)\n",
    "\n",
    "        q = q.view(bs, seq_len, self.n_heads, self.head_dim)\n",
    "        k = k.view(bs, seq_len, self.n_kv_heads, self.head_dim)\n",
    "        v = v.view(bs, seq_len, self.n_kv_heads, self.head_dim)\n",
    "\n",
    "        q = apply_rotary_emb(q, freqs_cis=freqs_cis)\n",
    "        k = apply_rotary_emb(k, freqs_cis=freqs_cis)\n",
    "\n",
    "        q, k, v = map(lambda x: x.transpose(1, 2), (q, k, v))\n",
    "\n",
    "        if cache is not None:\n",
    "            raise NotImplementedError # TODO\n",
    "\n",
    "        k = k.repeat_interleave(self.repeats, dim=1)\n",
    "        v = v.repeat_interleave(self.repeats, dim=1)\n",
    "\n",
    "        attn_weights = torch.matmul(q, k.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        if mask is not None:\n",
    "            raise NotImplementedError # TODO\n",
    "\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        output = torch.matmul(attn_weights, v).transpose(1, 2).contiguous()\n",
    "        output = output.view(bs, seq_len, self.n_heads * self.head_dim)\n",
    "        return self.wo(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1dc8cfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 2\n",
    "seq_len = 16\n",
    "dim = 512\n",
    "n_heads = 8\n",
    "head_dim = dim // n_heads\n",
    "inputs = torch.randn((bs, seq_len, dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "510f7827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 32, 2])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs_cis = precompute_freqs_cis(seq_len=seq_len, head_dim=head_dim)\n",
    "freqs_cis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0b53dc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "gqa = GroupedQueryAttention(dim, n_heads, head_dim, n_kv_heads=4, dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e1feb139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 512])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gqa(inputs, freqs_cis).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0fb10ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedFeedForward(nn.Module):\n",
    "    def __init__(self, dim: int, hidden_dim: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n",
    "        self.wg = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        self.act_fn = nn.SiLU()\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.act_fn(self.w1(inputs)) # up\n",
    "        hidden_states = hidden_states * self.wg(inputs) # gate\n",
    "        hidden_states = self.w2(hidden_states) # down\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9d40c8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b50f71b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoELayer(nn.Module):\n",
    "    def __init__(self, dim: int, hidden_dim: int, n_experts: int, n_experts_per_tok: int = 2) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_experts = n_experts\n",
    "        self.n_experts_per_tok = n_experts_per_tok\n",
    "\n",
    "        # gating, router, routing function (dim -> num_experts)\n",
    "        self.gate = nn.Linear(dim, n_experts, bias=False)\n",
    "        self.experts = nn.ModuleList([\n",
    "            GatedFeedForward(dim=dim, hidden_dim=hidden_dim) for _ in range(n_experts)\n",
    "        ])\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        batch_size, seq_len, dim = inputs.shape\n",
    "\n",
    "        inputs = inputs.view(-1, dim)  # (batch_size * seq_len, dim)\n",
    "        router_logits = self.gate(inputs) # (batch_size * seq_len, n_experts)\n",
    "\n",
    "        # routing_weights := ?, (batch_size * seq_len, num_experts_per_tok)\n",
    "        # selected_experts := indices of each chosen expert per token, (batch_size * seq_len, num_experts_per_tok)\n",
    "        routing_weights, selected_experts = torch.topk(router_logits, self.n_experts_per_tok)\n",
    "        routing_weights = F.softmax(routing_weights, dim=1, dtype=torch.float).to(inputs.dtype)\n",
    "\n",
    "        results = torch.zeros_like(inputs, dtype=inputs.dtype, device=inputs.device) # (batch_size * seq_len, dim)\n",
    "\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            batch_idx, nth_expert = torch.where(selected_experts == i)\n",
    "            results[batch_idx] += routing_weights[batch_idx, nth_expert, None] * expert(inputs[batch_idx])\n",
    "\n",
    "        results = results.reshape(batch_size, seq_len, dim)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2f8fc5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((2, 4, 4)) # (batch_size, seq_len, dim)\n",
    "moe_layer = MoELayer(dim=4, hidden_dim=8, n_experts=3, n_experts_per_tok=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0e49aced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 4])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moe_layer(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d64eda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, dim: int, hidden_dim: int, n_heads: int, n_kv_heads: int, head_dim: int, norm_eps: float, attn_dropout: float) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.dim = dim\n",
    "        self.attention = GroupedQueryAttention(\n",
    "            dim=dim, n_heads=n_heads, head_dim=head_dim, n_kv_heads=n_kv_heads, dropout=attn_dropout\n",
    "        )\n",
    "        self.attention_norm = RMSNorm(dim, eps=norm_eps)\n",
    "        self.ffn_norm = RMSNorm(dim, eps=norm_eps)\n",
    "\n",
    "        self.feed_forward: nn.Module\n",
    "        if moe is not None:\n",
    "            self.feed_forward = MoeLayer(\n",
    "                experts=[FeedForward(dim=dim, hidden_dim=hidden_dim, lora=lora) for _ in range(moe.num_experts)],\n",
    "                gate=nn.Linear(dim, moe.num_experts, bias=False),\n",
    "                moe_args=moe,\n",
    "            )\n",
    "        else:\n",
    "            self.feed_forward = FeedForward(dim=dim, hidden_dim=hidden_dim, lora=lora)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        freqs_cis: torch.Tensor,\n",
    "        cache: Optional[CacheView] = None,\n",
    "        mask: Optional[BlockDiagonalMask] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        r = self.attention.forward(self.attention_norm(x), freqs_cis, cache)\n",
    "        h = x + r\n",
    "        r = self.feed_forward.forward(self.ffn_norm(h))\n",
    "        out = h + r\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30067577",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
